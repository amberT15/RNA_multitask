{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dna_tokenizer import DNATokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   2,  448, 1595,  448, 1595,  448, 1595,    3]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = DNATokenizer('./vocab.txt')\n",
    "tokenizer.encode_plus('ATCGCG TCAGTC ATCGCG TCAGTC ATCGCG TCAGTC', return_tensors = 'pt',add_special_tokens = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   2,  448, 1595,  448, 1595,    3],\n",
       "         [   2,  448, 1595,    3,    0,    0],\n",
       "         [   2,  448, 1595,  448, 1595,    3],\n",
       "         [   2,  448, 1595,    3,    0,    0]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 1, 1]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 0, 0]])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = DNATokenizer('./vocab.txt',max_len=6)\n",
    "tokenizer.batch_encode_plus(['ATCGCG TCAGTC ATCGCG TCAGTC ATCGCG TCAGTC','ATCGCG TCAGTC ATCGCG TCAGTC ATCGCG TCAGTC'],max_length = 6, \n",
    "                            context_split=True,\n",
    "                            return_tensors = 'pt',\n",
    "                            add_special_tokens=True,\n",
    "                            return_attention_masks=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers.tokenization_utils import AddedToken,PreTrainedTokenizer\n",
    "from transformers.utils import is_tf_available, is_torch_available\n",
    "import collections\n",
    "import unicodedata\n",
    "import logging\n",
    "import itertools\n",
    "import re\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab(vocab_file):\n",
    "    \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n",
    "    vocab = collections.OrderedDict()\n",
    "    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n",
    "        tokens = reader.readlines()\n",
    "    for index, token in enumerate(tokens):\n",
    "        token = token.rstrip(\"\\n\")\n",
    "        vocab[token] = index\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   2,  448, 1595,  448, 1595,  448, 1595,    3],\n",
       "         [   2,  448, 1595,  448, 1595,  448,    3,    0]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 1]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 0]])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dnabert_datastruct import DNATokenizer as oldtoken\n",
    "compare = oldtoken('./vocab.txt')\n",
    "compare.batch_encode_plus(['ATCGCG TCAGTC ATCGCG TCAGTC ATCGCG TCAGTC','ATCGCG TCAGTC ATCGCG TCAGTC ATCGCG'],\n",
    "                            return_tensors = 'pt',\n",
    "                            add_special_tokens = True,\n",
    "                            return_attention_masks=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_FILES_NAMES = {\"vocab_file\": \"vocab.txt\"}\n",
    "PRETRAINED_VOCAB_FILES_MAP = {\"vocab_file\": {\"dna3\": \"https://raw.githubusercontent.com/jerryji1993/DNABERT/master/src/transformers/dnabert-config/bert-config-3/vocab.txt\",\n",
    "                                             \"dna4\": \"https://raw.githubusercontent.com/jerryji1993/DNABERT/master/src/transformers/dnabert-config/bert-config-4/vocab.txt\",\n",
    "                                             \"dna5\": \"https://raw.githubusercontent.com/jerryji1993/DNABERT/master/src/transformers/dnabert-config/bert-config-5/vocab.txt\",\n",
    "                                             \"dna6\": \"https://raw.githubusercontent.com/jerryji1993/DNABERT/master/src/transformers/dnabert-config/bert-config-6/vocab.txt\"}}\n",
    "PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {\n",
    "                                          \"dna3\": 512,\n",
    "                                          \"dna4\": 512,\n",
    "                                          \"dna5\": 512,\n",
    "                                          \"dna6\": 512}\n",
    "PRETRAINED_INIT_CONFIGURATION = {\n",
    "    \"dna3\": {\"do_lower_case\": False},\n",
    "    \"dna4\": {\"do_lower_case\": False},\n",
    "    \"dna5\": {\"do_lower_case\": False},\n",
    "    \"dna6\": {\"do_lower_case\": False}}\n",
    "VOCAB_KMER = {\n",
    "    \"69\": \"3\",\n",
    "    \"261\": \"4\",\n",
    "    \"1029\": \"5\",\n",
    "    \"4101\": \"6\",}\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class DNATokenizer(PreTrainedTokenizer):\n",
    "\n",
    "    vocab_files_names = VOCAB_FILES_NAMES\n",
    "    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP\n",
    "    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\n",
    "    model_input_names = [\"input_ids\", \"attention_mask\"]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_file,\n",
    "        max_len = 512,\n",
    "        never_split=None,\n",
    "        unk_token=\"[UNK]\",\n",
    "        sep_token=\"[SEP]\",\n",
    "        pad_token=\"[PAD]\",\n",
    "        cls_token=\"[CLS]\",\n",
    "        mask_token=\"[MASK]\",\n",
    "        **kwargs\n",
    "    ):\n",
    "        \n",
    "        super().__init__(\n",
    "            max_len = 512,\n",
    "            unk_token=unk_token,\n",
    "            sep_token=sep_token,\n",
    "            pad_token=pad_token,\n",
    "            cls_token=cls_token,\n",
    "            mask_token=mask_token,\n",
    "            **kwargs,\n",
    "        )\n",
    "        self.max_len = max_len \n",
    "        if not os.path.isfile(vocab_file):\n",
    "            raise ValueError(\n",
    "                \"Can't find a vocabulary file at path '{}'. To load the vocabulary from a Google pretrained \"\n",
    "                \"model use `tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`\".format(vocab_file)\n",
    "            )\n",
    "        self.vocab = load_vocab(vocab_file)\n",
    "        self.ids_to_tokens = collections.OrderedDict([(ids, tok) for tok, ids in self.vocab.items()])\n",
    "        self.kmer = VOCAB_KMER[str(len(self.vocab))]\n",
    "\n",
    "    def prepare_for_tokenization(self, text, **kwargs):\n",
    "        \"\"\" Performs any necessary transformations before tokenization \"\"\"\n",
    "        return text\n",
    "    \n",
    "    def tokenize(self, text, **kwargs):\n",
    "        \"\"\" Converts a string in a sequence of tokens (string), using the tokenizer.\n",
    "            Split in words for word-based vocabulary or sub-words for sub-word-based\n",
    "            vocabularies (BPE/SentencePieces/WordPieces).\n",
    "\n",
    "            Take care of added tokens.\n",
    "\n",
    "            text: The sequence to be encoded.\n",
    "            add_prefix_space: Only applies to GPT-2 and RoBERTa tokenizers. When `True`, this ensures that the sequence\n",
    "                begins with an empty space. False by default except for when using RoBERTa with `add_special_tokens=True`.\n",
    "            **kwargs: passed to the `prepare_for_tokenization` preprocessing method.\n",
    "        \"\"\"\n",
    "        all_special_tokens = self.all_special_tokens\n",
    "        text = self.prepare_for_tokenization(text, **kwargs)\n",
    "\n",
    "        def lowercase_text(t):\n",
    "            # convert non-special tokens to lowercase\n",
    "            escaped_special_toks = [re.escape(s_tok) for s_tok in all_special_tokens]\n",
    "            pattern = r\"(\" + r\"|\".join(escaped_special_toks) + r\")|\" + r\"(.+?)\"\n",
    "            return re.sub(pattern, lambda m: m.groups()[0] or m.groups()[1].lower(), t)\n",
    "\n",
    "        if self.init_kwargs.get(\"do_lower_case\", False):\n",
    "            text = lowercase_text(text)\n",
    "\n",
    "        def split_on_token(tok, text):\n",
    "            result = []\n",
    "            split_text = text.split(tok)\n",
    "            for i, sub_text in enumerate(split_text):\n",
    "                sub_text = sub_text.rstrip()\n",
    "                if i == 0 and not sub_text:\n",
    "                    result += [tok]\n",
    "                elif i == len(split_text) - 1:\n",
    "                    if sub_text:\n",
    "                        result += [sub_text]\n",
    "                    else:\n",
    "                        pass\n",
    "                else:\n",
    "                    if sub_text:\n",
    "                        result += [sub_text]\n",
    "                    result += [tok]\n",
    "            return result\n",
    "\n",
    "        def split_on_tokens(tok_list, text):\n",
    "            if not text.strip():\n",
    "                return []\n",
    "            if not tok_list:\n",
    "                return self._tokenize(text)\n",
    "\n",
    "            tokenized_text = []\n",
    "            text_list = [text]\n",
    "            for tok in tok_list:\n",
    "                tokenized_text = []\n",
    "                for sub_text in text_list:\n",
    "                    if sub_text not in self.all_special_tokens:\n",
    "                        tokenized_text += split_on_token(tok, sub_text)\n",
    "                    else:\n",
    "                        tokenized_text += [sub_text]\n",
    "                text_list = tokenized_text\n",
    "\n",
    "            return list(\n",
    "                itertools.chain.from_iterable(\n",
    "                    (\n",
    "                        self._tokenize(token) if token not in self.all_special_tokens else [token]\n",
    "                        for token in tokenized_text\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "\n",
    "        added_tokens = self.all_special_tokens\n",
    "        tokenized_text = split_on_tokens(added_tokens, text)\n",
    "        return tokenized_text\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "        text = self._clean_text(text)\n",
    "        split_tokens = text.strip().split()\n",
    "        \n",
    "        return split_tokens       \n",
    "\n",
    "    def _convert_token_to_id(self, token):\n",
    "        \"\"\"Converts a token (str) in an id using the vocab.\"\"\"\n",
    "        return self.vocab.get(token, self.vocab.get(self.unk_token))\n",
    "\n",
    "    def _convert_id_to_token(self, index):\n",
    "        \"\"\"Converts an index (integer) in a token (str) using the vocab.\"\"\"\n",
    "        return self.ids_to_tokens.get(index, self.unk_token)\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "        \"\"\" Converts a sequence of tokens (string) in a single string. \"\"\"\n",
    "        out_string = \" \".join(tokens).replace(\" ##\", \"\").strip()\n",
    "        return out_string\n",
    "\n",
    "    def encode_plus(self,\n",
    "        text,\n",
    "        text_pair=None,\n",
    "        add_special_tokens=True,\n",
    "        max_length=None,\n",
    "        stride=0,\n",
    "        truncation=\"longest_first\",\n",
    "        padding=False,\n",
    "        return_tensors=None,\n",
    "        return_token_type_ids=True,\n",
    "        return_attention_mask=True,\n",
    "        return_overflowing_tokens=False,\n",
    "        return_special_tokens_mask=False,\n",
    "        return_offsets_mapping=False,\n",
    "        **kwargs):\n",
    "\n",
    "        def get_input_ids(text):\n",
    "            if isinstance(text, str):\n",
    "                tokens = self.tokenize(text, add_special_tokens=add_special_tokens, **kwargs)\n",
    "                return self.convert_tokens_to_ids(tokens)\n",
    "            elif isinstance(text, (list, tuple)) and len(text) > 0 and isinstance(text[0], str):\n",
    "                return self.convert_tokens_to_ids(text)\n",
    "            elif isinstance(text, (list, tuple)) and len(text) > 0 and isinstance(text[0], int):\n",
    "                return text\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"Input is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers.\"\n",
    "                )\n",
    "        if return_offsets_mapping:\n",
    "            raise NotImplementedError(\n",
    "                \"return_offset_mapping is not available when using Python tokenizers.\"\n",
    "                \"To use this feature, change your tokenizer to one deriving from \"\n",
    "                \"transformers.PreTrainedTokenizerFast.\"\n",
    "                \"More information on available tokenizers at \"\n",
    "                \"https://github.com/huggingface/transformers/pull/2674\"\n",
    "            )\n",
    "\n",
    "        first_ids = get_input_ids(text)\n",
    "        second_ids = get_input_ids(text_pair) if text_pair is not None else None\n",
    "        context_split =  kwargs.get('context_split',False)\n",
    "        if context_split == False:\n",
    "            return self.prepare_for_model(\n",
    "                first_ids,\n",
    "                pair_ids=second_ids,\n",
    "                max_length=max_length,\n",
    "                pad_to_max_length=padding,\n",
    "                add_special_tokens=add_special_tokens,\n",
    "                stride=stride,\n",
    "                truncation_strategy=truncation,\n",
    "                return_tensors=return_tensors,\n",
    "                return_attention_mask=return_attention_mask,\n",
    "                return_token_type_ids=return_token_type_ids,\n",
    "                return_overflowing_tokens=return_overflowing_tokens,\n",
    "                return_special_tokens_mask=return_special_tokens_mask,\n",
    "            )\n",
    "        elif context_split == True:\n",
    "            pair =( second_ids != None)\n",
    "            encoded_inputs =  self.prep_context_split(\n",
    "                first_ids,\n",
    "                pair_ids=second_ids,\n",
    "                max_length=max_length ,\n",
    "                pad_to_max_length=padding,\n",
    "                add_special_tokens=add_special_tokens,\n",
    "                stride=stride,\n",
    "                truncation_strategy=truncation,\n",
    "                return_tensors=return_tensors,\n",
    "                return_attention_mask=return_attention_mask,\n",
    "                return_token_type_ids=return_token_type_ids,\n",
    "                return_overflowing_tokens=return_overflowing_tokens,\n",
    "                return_special_tokens_mask=return_special_tokens_mask,\n",
    "            )\n",
    "            return encoded_inputs\n",
    "\n",
    "    def prep_context_split(self,\n",
    "        ids,\n",
    "        pair_ids=None,\n",
    "        max_length=None,\n",
    "        add_special_tokens=True,\n",
    "        stride=0,\n",
    "        truncation_strategy=\"longest_first\",\n",
    "        pad_to_max_length=False,\n",
    "        return_tensors=None,\n",
    "        return_token_type_ids=True,\n",
    "        return_attention_mask=True,\n",
    "        return_overflowing_tokens=False,\n",
    "        return_special_tokens_mask=False,\n",
    "        ):\n",
    "\n",
    "        pair = bool(pair_ids is not None)\n",
    "        len_ids = len(ids)\n",
    "        len_pair_ids = len(pair_ids) if pair else 0\n",
    "        encoded_inputs={}\n",
    "\n",
    "        # Handle max sequence length\n",
    "        if max_length == None:  max_length = self.max_len\n",
    "        max_length = max_length - self.num_special_tokens_to_add(pair=pair) \n",
    "        total_len = len_ids + len_pair_ids + (self.num_special_tokens_to_add(pair=pair) if add_special_tokens else 0)\n",
    "        if max_length and total_len > max_length:\n",
    "            ids = [ids[i:min([i+max_length,len_ids])] for i in range(0, len_ids, max_length)]\n",
    "            pair_ids = ([pair_ids[i:min([i+max_length,len_pair_ids])] for i in range(0, len_pair_ids, max_length)] if pair else None)\n",
    "        else:\n",
    "            ids = [ids]\n",
    "            pair_ids = [pair_ids]\n",
    "        seq_list = []\n",
    "        token_list = []\n",
    "        type_list = []\n",
    "        attention_list = []\n",
    "        for i in range(0,len(ids)):\n",
    "            i_seq = ids[i]\n",
    "            pair_seq = pair_ids[i] if pair else None\n",
    "            \n",
    "            #Handle special_tokens \n",
    "            if add_special_tokens:\n",
    "                sequence = self.build_inputs_with_special_tokens(i_seq, pair_seq)\n",
    "                token_type_ids = self.create_token_type_ids_from_sequences(i_seq, pair_seq)\n",
    "            else: \n",
    "                sequence = i_seq + pair_seq if pair else i_seq\n",
    "                token_type_ids = [0] * len(i_seq) + ([1] * len(pair_seq) if pair else [])\n",
    "            \n",
    "            #Handle token mask\n",
    "            if return_special_tokens_mask:\n",
    "                token_mask = self.get_special_tokens_mask(i_seq, pair_seq)\n",
    "                           \n",
    "            #Handle attention mask\n",
    "            needs_to_be_padded = pad_to_max_length and (\n",
    "                max_length\n",
    "                and len(sequence) < max_length\n",
    "                or max_length is None\n",
    "                and len(sequence) < self.max_len\n",
    "                and self.max_len <= 10000\n",
    "            )\n",
    "            if needs_to_be_padded:\n",
    "                difference = (max_length if max_length is not None else self.max_len) - len(sequence)\n",
    "\n",
    "                if self.padding_side == \"right\":\n",
    "                    if return_attention_mask:\n",
    "                        att_mask = [1] * len(sequence) + [0] * difference\n",
    "                    if return_token_type_ids:\n",
    "                        token_type_ids = (\n",
    "                            token_type_ids + [self.pad_token_type_id] * difference\n",
    "                        )\n",
    "                    if return_special_tokens_mask:\n",
    "                        token_mask = token_mask + [1] * difference\n",
    "                    sequence = sequence + [self.pad_token_id] * difference\n",
    "                \n",
    "                elif self.padding_side == \"left\":\n",
    "                    if return_attention_mask:\n",
    "                        att_mask = [0] * difference + [1] * len(sequence)\n",
    "                    if return_token_type_ids:\n",
    "                        token_type_ids = [self.pad_token_type_id] * difference + token_type_ids\n",
    "                    if return_special_tokens_mask:\n",
    "                        token_mask = [1] * difference + token_mask\n",
    "                    sequence = [self.pad_token_id] * difference + sequence\n",
    "                \n",
    "                else:\n",
    "                    raise ValueError(\"Invalid padding strategy:\" + str(self.padding_side))\n",
    "            elif return_attention_mask:\n",
    "                att_mask = [1] * len(sequence)\n",
    "            \n",
    "            seq_list.append(sequence)\n",
    "            if return_token_type_ids:\n",
    "                type_list.append(token_type_ids)\n",
    "            if return_special_tokens_mask:\n",
    "                token_list.append(token_mask)\n",
    "            if return_attention_mask:    \n",
    "                attention_list.append(att_mask)\n",
    "\n",
    "        encoded_inputs[\"input_ids\"]=seq_list\n",
    "        if return_special_tokens_mask: encoded_inputs[\"special_tokens_mask\"] = token_list\n",
    "        if return_token_type_ids: encoded_inputs[\"token_type_ids\"] = type_list\n",
    "        if return_attention_mask: encoded_inputs[\"attention_mask\"] = attention_list\n",
    "        return encoded_inputs\n",
    "        \n",
    "    def prepare_for_model(\n",
    "        self,\n",
    "        ids,\n",
    "        pair_ids=None,\n",
    "        max_length=None,\n",
    "        add_special_tokens=True,\n",
    "        stride=0,\n",
    "        truncation_strategy=\"longest_first\",\n",
    "        pad_to_max_length=False,\n",
    "        return_tensors=None,\n",
    "        return_token_type_ids=True,\n",
    "        return_attention_mask=True,\n",
    "        return_overflowing_tokens=False,\n",
    "        return_special_tokens_mask=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Prepares a sequence of input id, or a pair of sequences of inputs ids so that it can be used by the model.\n",
    "        It adds special tokens, truncates\n",
    "        sequences if overflowing while taking into account the special tokens and manages a window stride for\n",
    "        overflowing tokens\n",
    "\n",
    "        Args:\n",
    "            ids: list of tokenized input ids. Can be obtained from a string by chaining the\n",
    "                `tokenize` and `convert_tokens_to_ids` methods.\n",
    "            pair_ids: Optional second list of input ids. Can be obtained from a string by chaining the\n",
    "                `tokenize` and `convert_tokens_to_ids` methods.\n",
    "            max_length: maximum length of the returned list. Will truncate by taking into account the special tokens.\n",
    "            add_special_tokens: if set to ``True``, the sequences will be encoded with the special tokens relative\n",
    "                to their model.\n",
    "            stride: window stride for overflowing tokens. Can be useful for edge effect removal when using sequential\n",
    "                list of inputs.\n",
    "            truncation_strategy: string selected in the following options:\n",
    "                - 'longest_first' (default) Iteratively reduce the inputs sequence until the input is under max_length\n",
    "                    starting from the longest one at each token (when there is a pair of input sequences)\n",
    "                - 'only_first': Only truncate the first sequence\n",
    "                - 'only_second': Only truncate the second sequence\n",
    "                - False: Does not truncate (raise an error if the input sequence is longer than max_length)\n",
    "            pad_to_max_length: if set to True, the returned sequences will be padded according to the model's padding side and\n",
    "                padding index, up to their max length. If no max length is specified, the padding is done up to the model's max length.\n",
    "                The tokenizer padding sides are handled by the following strings:\n",
    "                - 'left': pads on the left of the sequences\n",
    "                - 'right': pads on the right of the sequences\n",
    "                Defaults to False: no padding.\n",
    "            return_tensors: (optional) can be set to 'tf' or 'pt' to return respectively TensorFlow tf.constant\n",
    "                or PyTorch torch.Tensor instead of a list of python integers.\n",
    "            return_token_type_ids: (optional) Set to False to avoid returning token_type_ids (default True).\n",
    "            return_attention_mask: (optional) Set to False to avoid returning attention mask (default True)\n",
    "            return_overflowing_tokens: (optional) Set to True to return overflowing token information (default False).\n",
    "            return_special_tokens_mask: (optional) Set to True to return special tokens mask information (default False).\n",
    "\n",
    "        Return:\n",
    "            A Dictionary of shape::\n",
    "\n",
    "                {\n",
    "                    input_ids: list[int],\n",
    "                    token_type_ids: list[int] if return_token_type_ids is True (default)\n",
    "                    overflowing_tokens: list[int] if a ``max_length`` is specified and return_overflowing_tokens is True\n",
    "                    num_truncated_tokens: int if a ``max_length`` is specified and return_overflowing_tokens is True\n",
    "                    special_tokens_mask: list[int] if ``add_special_tokens`` if set to ``True`` and return_special_tokens_mask is True\n",
    "                }\n",
    "\n",
    "            With the fields:\n",
    "                ``input_ids``: list of token ids to be fed to a model\n",
    "                ``token_type_ids``: list of token type ids to be fed to a model\n",
    "\n",
    "                ``overflowing_tokens``: list of overflowing tokens if a max length is specified.\n",
    "                ``num_truncated_tokens``: number of overflowing tokens a ``max_length`` is specified\n",
    "                ``special_tokens_mask``: if adding special tokens, this is a list of [0, 1], with 0 specifying special added\n",
    "                tokens and 1 specifying sequence tokens.\n",
    "        \"\"\"\n",
    "        pair = bool(pair_ids is not None)\n",
    "        len_ids = len(ids)\n",
    "        len_pair_ids = len(pair_ids) if pair else 0\n",
    "\n",
    "        encoded_inputs = {}\n",
    "        # Handle max sequence length\n",
    "        total_len = len_ids + len_pair_ids + (self.num_special_tokens_to_add(pair=pair) if add_special_tokens else 0)\n",
    "        if max_length == None:  max_length = self.max_len \n",
    "        if max_length and total_len > max_length:\n",
    "            ids, pair_ids, overflowing_tokens = self.truncate_sequences(\n",
    "                ids,\n",
    "                pair_ids=pair_ids,\n",
    "                num_tokens_to_remove=total_len - max_length,\n",
    "                truncation_strategy=truncation_strategy,\n",
    "                stride=stride,\n",
    "            )\n",
    "            if return_overflowing_tokens:\n",
    "                encoded_inputs[\"overflowing_tokens\"] = overflowing_tokens\n",
    "                encoded_inputs[\"num_truncated_tokens\"] = total_len - max_length\n",
    "\n",
    "        # Handle special_tokens\n",
    "        if add_special_tokens:\n",
    "            sequence = self.build_inputs_with_special_tokens(ids, pair_ids)\n",
    "            token_type_ids = self.create_token_type_ids_from_sequences(ids, pair_ids)\n",
    "        else:\n",
    "            sequence = ids + pair_ids if pair else ids\n",
    "            token_type_ids = [0] * len(ids) + ([1] * len(pair_ids) if pair else [])\n",
    "\n",
    "        if return_special_tokens_mask:\n",
    "            encoded_inputs[\"special_tokens_mask\"] = self.get_special_tokens_mask(ids, pair_ids)\n",
    "\n",
    "        encoded_inputs[\"input_ids\"] = sequence\n",
    "        if return_token_type_ids:\n",
    "            encoded_inputs[\"token_type_ids\"] = token_type_ids\n",
    "\n",
    "        if max_length and len(encoded_inputs[\"input_ids\"]) > max_length:\n",
    "            encoded_inputs[\"input_ids\"] = encoded_inputs[\"input_ids\"][:max_length]\n",
    "            if return_token_type_ids:\n",
    "                encoded_inputs[\"token_type_ids\"] = encoded_inputs[\"token_type_ids\"][:max_length]\n",
    "            if return_special_tokens_mask:\n",
    "                encoded_inputs[\"special_tokens_mask\"] = encoded_inputs[\"special_tokens_mask\"][:max_length]\n",
    "\n",
    "        if max_length is None and len(encoded_inputs[\"input_ids\"]) > self.max_len:\n",
    "            logger.warning(\n",
    "                \"Token indices sequence length is longer than the specified maximum sequence length \"\n",
    "                \"for this model ({} > {}). Running this sequence through the model will result in \"\n",
    "                \"indexing errors\".format(len(ids), self.max_len)\n",
    "            )\n",
    "\n",
    "        needs_to_be_padded = pad_to_max_length and (\n",
    "            max_length\n",
    "            and len(encoded_inputs[\"input_ids\"]) < max_length\n",
    "            or max_length is None\n",
    "            and len(encoded_inputs[\"input_ids\"]) < self.max_len\n",
    "            and self.max_len <= 10000\n",
    "        )\n",
    "\n",
    "        if pad_to_max_length and max_length is None and self.max_len > 10000:\n",
    "            logger.warning(\n",
    "                \"Sequence can't be padded as no maximum length is specified and the model maximum length is too high.\"\n",
    "            )\n",
    "\n",
    "        if needs_to_be_padded:\n",
    "            difference = (max_length if max_length is not None else self.max_len) - len(encoded_inputs[\"input_ids\"])\n",
    "\n",
    "            if self.padding_side == \"right\":\n",
    "                if return_attention_mask:\n",
    "                    encoded_inputs[\"attention_mask\"] = [1] * len(encoded_inputs[\"input_ids\"]) + [0] * difference\n",
    "                if return_token_type_ids:\n",
    "                    encoded_inputs[\"token_type_ids\"] = (\n",
    "                        encoded_inputs[\"token_type_ids\"] + [self.pad_token_type_id] * difference\n",
    "                    )\n",
    "                if return_special_tokens_mask:\n",
    "                    encoded_inputs[\"special_tokens_mask\"] = encoded_inputs[\"special_tokens_mask\"] + [1] * difference\n",
    "                encoded_inputs[\"input_ids\"] = encoded_inputs[\"input_ids\"] + [self.pad_token_id] * difference\n",
    "            elif self.padding_side == \"left\":\n",
    "                if return_attention_mask:\n",
    "                    encoded_inputs[\"attention_mask\"] = [0] * difference + [1] * len(encoded_inputs[\"input_ids\"])\n",
    "                if return_token_type_ids:\n",
    "                    encoded_inputs[\"token_type_ids\"] = [self.pad_token_type_id] * difference + encoded_inputs[\n",
    "                        \"token_type_ids\"\n",
    "                    ]\n",
    "                if return_special_tokens_mask:\n",
    "                    encoded_inputs[\"special_tokens_mask\"] = [1] * difference + encoded_inputs[\"special_tokens_mask\"]\n",
    "                encoded_inputs[\"input_ids\"] = [self.pad_token_id] * difference + encoded_inputs[\"input_ids\"]\n",
    "\n",
    "            else:\n",
    "                raise ValueError(\"Invalid padding strategy:\" + str(self.padding_side))\n",
    "\n",
    "        elif return_attention_mask:\n",
    "            encoded_inputs[\"attention_mask\"] = [1] * len(encoded_inputs[\"input_ids\"])\n",
    "\n",
    "        # Prepare inputs as tensors if asked\n",
    "        if return_tensors == \"tf\" and is_tf_available():\n",
    "            encoded_inputs[\"input_ids\"] = tf.constant([encoded_inputs[\"input_ids\"]])\n",
    "\n",
    "            if \"token_type_ids\" in encoded_inputs:\n",
    "                encoded_inputs[\"token_type_ids\"] = tf.constant([encoded_inputs[\"token_type_ids\"]])\n",
    "\n",
    "            if \"attention_mask\" in encoded_inputs:\n",
    "                encoded_inputs[\"attention_mask\"] = tf.constant([encoded_inputs[\"attention_mask\"]])\n",
    "\n",
    "        elif return_tensors == \"pt\" and is_torch_available():\n",
    "            encoded_inputs[\"input_ids\"] = torch.tensor([encoded_inputs[\"input_ids\"]])\n",
    "\n",
    "            if \"token_type_ids\" in encoded_inputs:\n",
    "                encoded_inputs[\"token_type_ids\"] = torch.tensor([encoded_inputs[\"token_type_ids\"]])\n",
    "\n",
    "            if \"attention_mask\" in encoded_inputs:\n",
    "                encoded_inputs[\"attention_mask\"] = torch.tensor([encoded_inputs[\"attention_mask\"]])\n",
    "        elif return_tensors is not None:\n",
    "            logger.warning(\n",
    "                \"Unable to convert output to tensors format {}, PyTorch or TensorFlow is not available.\".format(\n",
    "                    return_tensors\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return encoded_inputs\n",
    "\n",
    "    def batch_encode_plus(\n",
    "        self,\n",
    "        batch_text_or_text_pairs=None,\n",
    "        add_special_tokens=False,\n",
    "        max_length=None,\n",
    "        stride=0,\n",
    "        truncation_strategy=\"longest_first\",\n",
    "        return_tensors=None,\n",
    "        return_input_lengths=False,\n",
    "        return_attention_masks=False,\n",
    "        return_offsets_mapping=False,\n",
    "        context_split=False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        if return_offsets_mapping:\n",
    "            raise NotImplementedError(\n",
    "                \"return_offset_mapping is not available when using Python tokenizers.\"\n",
    "                \"To use this feature, change your tokenizer to one deriving from \"\n",
    "                \"transformers.PreTrainedTokenizerFast.\"\n",
    "                \"More information on available tokenizers at \"\n",
    "                \"https://github.com/huggingface/transformers/pull/2674\"\n",
    "            )\n",
    "        \n",
    "        batch_outputs = {}\n",
    "        if context_split == True: \n",
    "            for ids_or_pair_ids in batch_text_or_text_pairs:\n",
    "                if isinstance(ids_or_pair_ids, (list, tuple)):\n",
    "                    assert len(ids_or_pair_ids) == 2\n",
    "                    ids, pair_ids = ids_or_pair_ids\n",
    "                else:\n",
    "                    ids, pair_ids = ids_or_pair_ids, None\n",
    "                outputs = self.encode_plus(\n",
    "                    ids,\n",
    "                    pair_ids,\n",
    "                    add_special_tokens=add_special_tokens,\n",
    "                    max_length=max_length,\n",
    "                    stride=stride,\n",
    "                    truncation_strategy=truncation_strategy,\n",
    "                    return_tensors=None,\n",
    "                    context_split=True\n",
    "                )\n",
    "                # Append the non-padded length to the output\n",
    "                if return_input_lengths:\n",
    "                    outputs[\"input_len\"] = len(outputs[\"input_ids\"])\n",
    "\n",
    "                for key, value in outputs.items():\n",
    "                    if key not in batch_outputs:\n",
    "                        batch_outputs[key] = []\n",
    "                    batch_outputs[key].extend(value)\n",
    "            \n",
    "        else:\n",
    "            for ids_or_pair_ids in batch_text_or_text_pairs:\n",
    "                if isinstance(ids_or_pair_ids, (list, tuple)):\n",
    "                    assert len(ids_or_pair_ids) == 2\n",
    "                    ids, pair_ids = ids_or_pair_ids\n",
    "                else:\n",
    "                    ids, pair_ids = ids_or_pair_ids, None\n",
    "                outputs = self.encode_plus(\n",
    "                    ids,\n",
    "                    pair_ids,\n",
    "                    add_special_tokens=add_special_tokens,\n",
    "                    max_length=max_length,\n",
    "                    stride=stride,\n",
    "                    truncation_strategy=truncation_strategy,\n",
    "                    return_tensors=None,\n",
    "                )\n",
    "\n",
    "                # Append the non-padded length to the output\n",
    "                if return_input_lengths:\n",
    "                    outputs[\"input_len\"] = len(outputs[\"input_ids\"])\n",
    "\n",
    "                for key, value in outputs.items():\n",
    "                    if key not in batch_outputs:\n",
    "                        batch_outputs[key] = []\n",
    "                    batch_outputs[key].append(value)\n",
    "\n",
    "        # Compute longest sequence size\n",
    "        max_seq_len = max(map(len, batch_outputs[\"input_ids\"]))\n",
    "        if return_attention_masks:\n",
    "            # Allow the model to not give any special attention to padded input\n",
    "            batch_outputs[\"attention_mask\"] = [[0] * len(v) for v in batch_outputs[\"input_ids\"]]\n",
    "\n",
    "        if return_tensors is not None:\n",
    "\n",
    "            # Do the tensor conversion in batch\n",
    "            for key, value in batch_outputs.items():\n",
    "\n",
    "                padded_value = value\n",
    "                # verify that the tokenizer has a pad_token_id\n",
    "                if key != \"input_len\" and self._pad_token is not None:\n",
    "                    # Padding handle\n",
    "                    padded_value = [\n",
    "                        v + [self.pad_token_id if key == \"input_ids\" else 1] * (max_seq_len - len(v))\n",
    "                        for v in padded_value\n",
    "                    ]\n",
    "\n",
    "                if return_tensors == \"tf\" and is_tf_available():\n",
    "                    batch_outputs[key] = tf.constant(padded_value)\n",
    "                elif return_tensors == \"pt\" and is_torch_available():\n",
    "                    batch_outputs[key] = torch.tensor(padded_value)\n",
    "                elif return_tensors is not None:\n",
    "                    logger.warning(\n",
    "                        \"Unable to convert output to tensors format {}, PyTorch or TensorFlow is not available.\".format(\n",
    "                            return_tensors\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "        # encoder_attention_mask requires 1 for real token, 0 for padding, just invert value\n",
    "        if return_attention_masks:\n",
    "            if is_torch_available():\n",
    "                batch_outputs[\"attention_mask\"] = torch.abs(batch_outputs[\"attention_mask\"] - 1)\n",
    "            else:\n",
    "                batch_outputs[\"attention_mask\"] = tf.abs(batch_outputs[\"attention_mask\"] - 1)\n",
    "                \n",
    "        return batch_outputs\n",
    "    \n",
    "    def save_vocabulary(self, vocab_path):\n",
    "\n",
    "        index = 0\n",
    "        if os.path.isdir(vocab_path):\n",
    "            vocab_file = os.path.join(vocab_path, VOCAB_FILES_NAMES[\"vocab_file\"])\n",
    "        else:\n",
    "            vocab_file = vocab_path\n",
    "        with open(vocab_file, \"w\", encoding=\"utf-8\") as writer:\n",
    "            for token, token_index in sorted(self.vocab.items(), key=lambda kv: kv[1]):\n",
    "                if index != token_index:\n",
    "                    logger.warning(\n",
    "                        \"Saving vocabulary to {}: vocabulary indices are not consecutive.\"\n",
    "                        \" Please check that the vocabulary is not corrupted!\".format(vocab_file)\n",
    "                    )\n",
    "                    index = token_index\n",
    "                writer.write(token + \"\\n\")\n",
    "                index += 1\n",
    "        return (vocab_file,)\n",
    "\n",
    "    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None): \n",
    "        \"\"\"\n",
    "        Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n",
    "        adding special tokens. A RoBERTa sequence has the following format:\n",
    "\n",
    "        - single sequence: `<s> X </s>`\n",
    "        - pair of sequences: `<s> A </s></s> B </s>`\n",
    "\n",
    "        Args:\n",
    "            token_ids_0 (`List[int]`):\n",
    "                List of IDs to which the special tokens will be added.\n",
    "            token_ids_1 (`List[int]`, *optional*):\n",
    "                Optional second list of IDs for sequence pairs.\n",
    "\n",
    "        Returns:\n",
    "            `List[int]`: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.\n",
    "        \"\"\"\n",
    "        if token_ids_1 is None:\n",
    "            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n",
    "        cls = [self.cls_token_id]\n",
    "        sep = [self.sep_token_id]\n",
    "        return cls + token_ids_0 + sep + sep + token_ids_1 + sep\n",
    "\n",
    "    def get_special_tokens_mask(self, token_ids_0, token_ids_1= None, already_has_special_tokens= False):\n",
    "        if already_has_special_tokens:\n",
    "            return super().get_special_tokens_mask(\n",
    "                token_ids_0=token_ids_0, token_ids_1=token_ids_1, already_has_special_tokens=True\n",
    "            )\n",
    "        if token_ids_1 is not None:\n",
    "            return [1] + ([0] * len(token_ids_0)) + [1, 1] + ([0] * len(token_ids_1)) + [1]\n",
    "        return [1] + ([0] * len(token_ids_0)) + [1]\n",
    "\n",
    "    def truncate_sequences(\n",
    "        self, ids, pair_ids=None, num_tokens_to_remove=0, truncation_strategy=\"longest_first\", stride=0\n",
    "    ):\n",
    "        \"\"\"Truncates a sequence pair in place to the maximum length.\n",
    "            truncation_strategy: string selected in the following options:\n",
    "                - 'longest_first' (default) Iteratively reduce the inputs sequence until the input is under max_length\n",
    "                    starting from the longest one at each token (when there is a pair of input sequences).\n",
    "                    Overflowing tokens only contains overflow from the first sequence.\n",
    "                - 'only_first': Only truncate the first sequence. raise an error if the first sequence is shorter or equal to than num_tokens_to_remove.\n",
    "                - 'only_second': Only truncate the second sequence\n",
    "                - False: Does not truncate (raise an error if the input sequence is longer than max_length)\n",
    "        \"\"\"\n",
    "        if num_tokens_to_remove <= 0:\n",
    "            return ids, pair_ids, []\n",
    "\n",
    "        if truncation_strategy == \"longest_first\":\n",
    "            overflowing_tokens = []\n",
    "            for _ in range(num_tokens_to_remove):\n",
    "                if pair_ids is None or len(ids) > len(pair_ids):\n",
    "                    overflowing_tokens = [ids[-1]] + overflowing_tokens\n",
    "                    ids = ids[:-1]\n",
    "                else:\n",
    "                    pair_ids = pair_ids[:-1]\n",
    "            window_len = min(len(ids), stride)\n",
    "            if window_len > 0:\n",
    "                overflowing_tokens = ids[-window_len:] + overflowing_tokens\n",
    "        elif truncation_strategy == \"only_first\":\n",
    "            assert len(ids) > num_tokens_to_remove\n",
    "            window_len = min(len(ids), stride + num_tokens_to_remove)\n",
    "            overflowing_tokens = ids[-window_len:]\n",
    "            ids = ids[:-num_tokens_to_remove]\n",
    "        elif truncation_strategy == \"only_second\":\n",
    "            assert pair_ids is not None and len(pair_ids) > num_tokens_to_remove\n",
    "            window_len = min(len(pair_ids), stride + num_tokens_to_remove)\n",
    "            overflowing_tokens = pair_ids[-window_len:]\n",
    "            pair_ids = pair_ids[:-num_tokens_to_remove]\n",
    "        elif truncation_strategy == False:\n",
    "            raise ValueError(\"Input sequence are too long for max_length. Please select a truncation strategy.\")\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Truncation_strategy should be selected in ['longest_first', 'only_first', 'only_second', False]\"\n",
    "            )\n",
    "        return (ids, pair_ids, overflowing_tokens)\n",
    "    \n",
    "    def create_token_type_ids_from_sequences(self, token_ids_0, token_ids_1=None):\n",
    "        \"\"\"\n",
    "        Creates a mask from the two sequences passed to be used in a sequence-pair classification task.\n",
    "        A BERT sequence pair mask has the following format:\n",
    "        0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
    "        | first sequence    | second sequence\n",
    "\n",
    "        if token_ids_1 is None, only returns the first portion of the mask (0's).\n",
    "        \"\"\"\n",
    "        sep = [self.sep_token_id]\n",
    "        cls = [self.cls_token_id]\n",
    "        if token_ids_1 is None:\n",
    "            if len(token_ids_0) < 510:\n",
    "                return len(cls + token_ids_0 + sep) * [0]\n",
    "            else:\n",
    "                num_pieces = int(len(token_ids_0)//510) + 1\n",
    "                return (len(cls + token_ids_0 + sep) + 2*(num_pieces-1)) * [0]\n",
    "        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.vocab)\n",
    "\n",
    "    @property\n",
    "    def bos_token_id(self):\n",
    "        \"\"\" Id of the beginning of sentence token in the vocabulary. Log an error if used while not having been set. \"\"\"\n",
    "        return self.convert_tokens_to_ids(self.bos_token)\n",
    "\n",
    "    @property\n",
    "    def eos_token_id(self):\n",
    "        \"\"\" Id of the end of sentence token in the vocabulary. Log an error if used while not having been set. \"\"\"\n",
    "        return self.convert_tokens_to_ids(self.eos_token)\n",
    "\n",
    "    @property\n",
    "    def unk_token_id(self):\n",
    "        \"\"\" Id of the unknown token in the vocabulary. Log an error if used while not having been set. \"\"\"\n",
    "        return self.convert_tokens_to_ids(self.unk_token)\n",
    "\n",
    "    @property\n",
    "    def sep_token_id(self):\n",
    "        \"\"\" Id of the separation token in the vocabulary. E.g. separate context and query in an input sequence. Log an error if used while not having been set. \"\"\"\n",
    "        return self.convert_tokens_to_ids(self.sep_token)\n",
    "\n",
    "    @property\n",
    "    def pad_token_id(self):\n",
    "        \"\"\" Id of the padding token in the vocabulary. Log an error if used while not having been set. \"\"\"\n",
    "        return self.convert_tokens_to_ids(self.pad_token)\n",
    "\n",
    "    @property\n",
    "    def pad_token_type_id(self):\n",
    "        \"\"\" Id of the padding token type in the vocabulary.\"\"\"\n",
    "        return self._pad_token_type_id\n",
    "\n",
    "    @property\n",
    "    def cls_token_id(self):\n",
    "        \"\"\" Id of the classification token in the vocabulary. E.g. to extract a summary of an input sequence leveraging self-attention along the full depth of the model. Log an error if used while not having been set. \"\"\"\n",
    "        return self.convert_tokens_to_ids(self.cls_token)\n",
    "\n",
    "    @property\n",
    "    def mask_token_id(self):\n",
    "        \"\"\" Id of the mask token in the vocabulary. E.g. when training a model with masked-language modeling. Log an error if used while not having been set. \"\"\"\n",
    "        return self.convert_tokens_to_ids(self.mask_token)\n",
    "\n",
    "    @property\n",
    "    def additional_special_tokens_ids(self):\n",
    "        \"\"\" Ids of all the additional special tokens in the vocabulary (list of integers). Log an error if used while not having been set. \"\"\"\n",
    "        return self.convert_tokens_to_ids(self.additional_special_tokens)\n",
    "\n",
    "    def _clean_text(self, text):\n",
    "            \"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"\n",
    "            output = []\n",
    "            for char in text:\n",
    "                cp = ord(char)\n",
    "                if cp == 0 or cp == 0xFFFD or self._is_control(char):\n",
    "                    continue\n",
    "                if self._is_whitespace(char):\n",
    "                    output.append(\" \")\n",
    "                else:\n",
    "                    output.append(char)\n",
    "            return \"\".join(output)\n",
    "        \n",
    "    def _is_whitespace(self,char):\n",
    "            \"\"\"Checks whether `chars` is a whitespace character.\"\"\"\n",
    "            # \\t, \\n, and \\r are technically contorl characters but we treat them\n",
    "            # as whitespace since they are generally considered as such.\n",
    "            if char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
    "                return True\n",
    "            cat = unicodedata.category(char)\n",
    "            if cat == \"Zs\":\n",
    "                return True\n",
    "            return False\n",
    "\n",
    "    def _is_control(self,char):\n",
    "            \"\"\"Checks whether `chars` is a control character.\"\"\"\n",
    "            # These are technically control characters but we count them as whitespace\n",
    "            # characters.\n",
    "            if char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
    "                return False\n",
    "            cat = unicodedata.category(char)\n",
    "            if cat.startswith(\"C\"):\n",
    "                return True\n",
    "            return False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('tf_2': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9e8284e1417b754e460c2bde3a4a4837c482fa82ceb7d52f4acbe340dd4b4559"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
