{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import torch\n",
    "import os\n",
    "import rna_model\n",
    "from torchinfo import summary\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = {\n",
    "    'model':'conv_former',\n",
    "    'hidden_size': 512,\n",
    "    'attention_window': [256,256,256,256,256,256],\n",
    "    'num_attention_heads': 8,\n",
    "    'intermediate_size': 2048,\n",
    "    'attention_dilation': [1,1,1,1,1,1],\n",
    "    'data_dir' : '/grid/koo/home/ztang/multitask_RNA/data/pre-train/3072/rna_onehot.h5',\n",
    "    'batch_size':16}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = rna_model.conv_former_config(**config_dict)\n",
    "model = rna_model.conv_former(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "conv_former                                   [16, 4, 3072]             --\n",
       "├─Conv1d: 1-1                                 [16, 512, 3072]           39,424\n",
       "├─BatchNorm1d: 1-2                            [16, 512, 3072]           1,024\n",
       "├─dilated_residual: 1-3                       [16, 512, 3072]           --\n",
       "│    └─Sequential: 2-1                        [16, 512, 3072]           --\n",
       "│    │    └─Conv1d: 3-1                       [16, 512, 3072]           1,835,520\n",
       "│    │    └─BatchNorm1d: 3-2                  [16, 512, 3072]           1,024\n",
       "│    │    └─ReLU: 3-3                         [16, 512, 3072]           --\n",
       "│    │    └─Dropout: 3-4                      [16, 512, 3072]           --\n",
       "│    │    └─Conv1d: 3-5                       [16, 512, 3072]           1,835,520\n",
       "│    │    └─BatchNorm1d: 3-6                  [16, 512, 3072]           1,024\n",
       "│    └─ReLU: 2-2                              [16, 512, 3072]           --\n",
       "├─dilated_residual: 1-4                       [16, 512, 3072]           --\n",
       "│    └─Sequential: 2-3                        [16, 512, 3072]           --\n",
       "│    │    └─Conv1d: 3-7                       [16, 512, 3072]           1,835,520\n",
       "│    │    └─BatchNorm1d: 3-8                  [16, 512, 3072]           1,024\n",
       "│    │    └─ReLU: 3-9                         [16, 512, 3072]           --\n",
       "│    │    └─Dropout: 3-10                     [16, 512, 3072]           --\n",
       "│    │    └─Conv1d: 3-11                      [16, 512, 3072]           1,835,520\n",
       "│    │    └─BatchNorm1d: 3-12                 [16, 512, 3072]           1,024\n",
       "│    └─ReLU: 2-4                              [16, 512, 3072]           --\n",
       "├─ModuleList: 1-5                             --                        --\n",
       "│    └─longformer_block: 2-5                  [16, 3072, 512]           --\n",
       "│    │    └─LongformerSelfAttention: 3-13     [16, 3072, 512]           1,575,936\n",
       "│    │    └─Linear: 3-14                      [16, 3072, 512]           262,656\n",
       "│    │    └─Dropout: 3-15                     [16, 3072, 512]           --\n",
       "│    │    └─LayerNorm: 3-16                   [16, 3072, 512]           1,024\n",
       "│    │    └─Linear: 3-17                      [16, 3072, 2048]          1,050,624\n",
       "│    │    └─GELU: 3-18                        [16, 3072, 2048]          --\n",
       "│    │    └─Linear: 3-19                      [16, 3072, 512]           1,049,088\n",
       "│    │    └─Dropout: 3-20                     [16, 3072, 512]           --\n",
       "│    │    └─LayerNorm: 3-21                   [16, 3072, 512]           1,024\n",
       "│    └─longformer_block: 2-6                  [16, 3072, 512]           --\n",
       "│    │    └─LongformerSelfAttention: 3-22     [16, 3072, 512]           1,575,936\n",
       "│    │    └─Linear: 3-24                      [16, 3072, 512]           (recursive)\n",
       "│    │    └─Linear: 3-24                      [16, 3072, 512]           (recursive)\n",
       "│    │    └─Dropout: 3-25                     [16, 3072, 512]           --\n",
       "│    │    └─LayerNorm: 3-26                   [16, 3072, 512]           1,024\n",
       "│    │    └─Linear: 3-27                      [16, 3072, 2048]          1,050,624\n",
       "│    │    └─GELU: 3-28                        [16, 3072, 2048]          --\n",
       "│    │    └─Linear: 3-29                      [16, 3072, 512]           1,049,088\n",
       "│    │    └─Dropout: 3-30                     [16, 3072, 512]           --\n",
       "│    │    └─LayerNorm: 3-31                   [16, 3072, 512]           1,024\n",
       "│    └─longformer_block: 2-7                  [16, 3072, 512]           --\n",
       "│    │    └─LongformerSelfAttention: 3-32     [16, 3072, 512]           1,575,936\n",
       "│    │    └─Linear: 3-33                      [16, 3072, 512]           262,656\n",
       "│    │    └─Dropout: 3-34                     [16, 3072, 512]           --\n",
       "│    │    └─LayerNorm: 3-35                   [16, 3072, 512]           1,024\n",
       "│    │    └─Linear: 3-36                      [16, 3072, 2048]          1,050,624\n",
       "│    │    └─GELU: 3-37                        [16, 3072, 2048]          --\n",
       "│    │    └─Linear: 3-38                      [16, 3072, 512]           1,049,088\n",
       "│    │    └─Dropout: 3-39                     [16, 3072, 512]           --\n",
       "│    │    └─LayerNorm: 3-40                   [16, 3072, 512]           1,024\n",
       "│    └─longformer_block: 2-8                  [16, 3072, 512]           --\n",
       "│    │    └─LongformerSelfAttention: 3-41     [16, 3072, 512]           1,575,936\n",
       "│    │    └─Linear: 3-43                      [16, 3072, 512]           (recursive)\n",
       "│    │    └─Linear: 3-43                      [16, 3072, 512]           (recursive)\n",
       "│    │    └─Dropout: 3-44                     [16, 3072, 512]           --\n",
       "│    │    └─LayerNorm: 3-45                   [16, 3072, 512]           1,024\n",
       "│    │    └─Linear: 3-46                      [16, 3072, 2048]          1,050,624\n",
       "│    │    └─GELU: 3-47                        [16, 3072, 2048]          --\n",
       "│    │    └─Linear: 3-48                      [16, 3072, 512]           1,049,088\n",
       "│    │    └─Dropout: 3-49                     [16, 3072, 512]           --\n",
       "│    │    └─LayerNorm: 3-50                   [16, 3072, 512]           1,024\n",
       "│    └─longformer_block: 2-9                  [16, 3072, 512]           --\n",
       "│    │    └─LongformerSelfAttention: 3-51     [16, 3072, 512]           1,575,936\n",
       "│    │    └─Linear: 3-52                      [16, 3072, 512]           262,656\n",
       "│    │    └─Dropout: 3-53                     [16, 3072, 512]           --\n",
       "│    │    └─LayerNorm: 3-54                   [16, 3072, 512]           1,024\n",
       "│    │    └─Linear: 3-55                      [16, 3072, 2048]          1,050,624\n",
       "│    │    └─GELU: 3-56                        [16, 3072, 2048]          --\n",
       "│    │    └─Linear: 3-57                      [16, 3072, 512]           1,049,088\n",
       "│    │    └─Dropout: 3-58                     [16, 3072, 512]           --\n",
       "│    │    └─LayerNorm: 3-59                   [16, 3072, 512]           1,024\n",
       "│    └─longformer_block: 2-10                 [16, 3072, 512]           --\n",
       "│    │    └─LongformerSelfAttention: 3-60     [16, 3072, 512]           1,575,936\n",
       "│    │    └─Linear: 3-62                      [16, 3072, 512]           (recursive)\n",
       "│    │    └─Linear: 3-62                      [16, 3072, 512]           (recursive)\n",
       "│    │    └─Dropout: 3-63                     [16, 3072, 512]           --\n",
       "│    │    └─LayerNorm: 3-64                   [16, 3072, 512]           1,024\n",
       "│    │    └─Linear: 3-65                      [16, 3072, 2048]          1,050,624\n",
       "│    │    └─GELU: 3-66                        [16, 3072, 2048]          --\n",
       "│    │    └─Linear: 3-67                      [16, 3072, 512]           1,049,088\n",
       "│    │    └─Dropout: 3-68                     [16, 3072, 512]           --\n",
       "│    │    └─LayerNorm: 3-69                   [16, 3072, 512]           1,024\n",
       "├─Sequential: 1-6                             [16, 3072, 512]           --\n",
       "│    └─Linear: 2-11                           [16, 3072, 512]           262,656\n",
       "│    └─GELU: 2-12                             [16, 3072, 512]           --\n",
       "│    └─LayerNorm: 2-13                        [16, 3072, 512]           1,024\n",
       "├─Linear: 1-7                                 [16, 3072, 4]             2,052\n",
       "├─CrossEntropyLoss: 1-8                       --                        --\n",
       "===============================================================================================\n",
       "Total params: 31,294,468\n",
       "Trainable params: 31,294,468\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 384.85\n",
       "===============================================================================================\n",
       "Input size (MB): 0.79\n",
       "Forward/backward pass size (MB): 13289.13\n",
       "Params size (MB): 99.96\n",
       "Estimated Total Size (MB): 13389.88\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, input_size=(16,4,3072))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
