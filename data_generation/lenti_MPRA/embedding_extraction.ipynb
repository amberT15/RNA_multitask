{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-15 15:20:03.564816: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2B5_model\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import sys\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = 'false'\n",
    "sys.path.append('/home/ztang/multitask_RNA/data_generation')\n",
    "import utils\n",
    "import numpy as np\n",
    "import haiku as hk\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from nucleotide_transformer.pretrained import get_pretrained_model\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import tensorflow as tf\n",
    "model_name = '2B5_1000G'\n",
    "if '2B5' in model_name:\n",
    "    print('2B5_model')\n",
    "    embed_layer = 32\n",
    "else:\n",
    "    print('500M model')\n",
    "    embed_layer = 24\n",
    "cell_name = 'HepG2'\n",
    "include_seq = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_bytes(values):\n",
    "  \"\"\"Convert numpy arrays to bytes features.\"\"\"\n",
    "  values = values.flatten().tostring()\n",
    "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[values]))\n",
    "\n",
    "def feature_str(values):\n",
    "  \"\"\"Convert str to bytes features.\"\"\"\n",
    "  # value = np.array(values)\n",
    "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[values]))\n",
    "\n",
    "def feature_floats(values):\n",
    "  \"\"\"Convert numpy arrays to floats features.\n",
    "     Requires more space than bytes.\"\"\"\n",
    "  values = values.flatten().tolist()\n",
    "  return tf.train.Feature(float_list=tf.train.FloatList(value=values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = h5py.File('/home/ztang/multitask_RNA/data/lenti_MPRA/'+cell_name+'_data.h5', 'r')\n",
    "sequence = data_file['seq'][()]\n",
    "target = data_file['mean'][()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters, forward_fn, tokenizer, config = get_pretrained_model(\n",
    "    model_name=model_name,\n",
    "    mixed_precision=False,\n",
    "    embeddings_layers_to_save=(embed_layer,),\n",
    "    attention_maps_to_save=(),\n",
    "    max_positions=41,\n",
    ")\n",
    "forward_fn = hk.transform(forward_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 139877/139877 [00:11<00:00, 12135.83it/s]\n"
     ]
    }
   ],
   "source": [
    "N,  = sequence.shape\n",
    "seq_pair = []\n",
    "seq_onehot = []\n",
    "for i in tqdm(range(N)):\n",
    "    seq = sequence[i].decode()\n",
    "    seq_onehot.append(utils.seq_to_onehot(seq))\n",
    "    token_out = tokenizer.batch_tokenize([seq])\n",
    "    token_id = [b[1] for b in token_out]\n",
    "    seq_pair.append(np.squeeze(token_id))\n",
    "#seq_pair = jnp.asarray(seq_pair,dtype=jnp.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 137/137 [08:32<00:00,  3.74s/it]\n"
     ]
    }
   ],
   "source": [
    "#get embedding per input sequence\n",
    "batch_size = 1024\n",
    "lenti_embed = []\n",
    "random_key = jax.random.PRNGKey(0)\n",
    "for i in tqdm(range(0, N, batch_size)):\n",
    "    seq_batch = jnp.asarray(seq_pair[i:i+batch_size],dtype=jnp.int32)\n",
    "    outs = forward_fn.apply(parameters, random_key, seq_batch)\n",
    "    lenti_embed.extend(np.array(outs['embeddings_'+str(embed_layer)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = np.array(target)\n",
    "lenti_embed = np.array(lenti_embed)\n",
    "seq_onehot = np.array(seq_onehot,dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving test\n",
      "tfr directory:  /home/ztang/multitask_RNA/data/lenti_MPRA_embed/HepG2_seq_2B5_1000G/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/28 [00:00<?, ?it/s]/tmp/ipykernel_2130212/3740337950.py:3: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  values = values.flatten().tostring()\n",
      "100%|██████████| 28/28 [01:29<00:00,  3.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving valid\n",
      "tfr directory:  /home/ztang/multitask_RNA/data/lenti_MPRA_embed/HepG2_seq_2B5_1000G/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [01:30<00:00,  3.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving train\n",
      "tfr directory:  /home/ztang/multitask_RNA/data/lenti_MPRA_embed/HepG2_seq_2B5_1000G/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 219/219 [12:02<00:00,  3.30s/it]\n"
     ]
    }
   ],
   "source": [
    "all_index = np.random.permutation(len(target))\n",
    "train_index = all_index[:int(0.8*len(target))]\n",
    "valid_index = all_index[int(0.8*len(target)):int(0.9*len(target))]\n",
    "test_index = all_index[int(0.9*len(target)):]\n",
    "num_samples = 512\n",
    "if include_seq:\n",
    "    tfr_dir = '/home/ztang/multitask_RNA/data/lenti_MPRA_embed/'+cell_name+'_seq_'+model_name+'/'\n",
    "else:\n",
    "    tfr_dir = '/home/ztang/multitask_RNA/data/lenti_MPRA_embed/'+cell_name+'_'+model_name+'/'\n",
    "\n",
    "for dataset in ['test','valid','train']:\n",
    "    print('saving '+dataset)\n",
    "    index = globals()[dataset+ '_index'] \n",
    "    sub_target = target[index]\n",
    "    sub_embed = lenti_embed[index]\n",
    "    if include_seq:\n",
    "        sub_seq = seq_onehot[index]\n",
    "\n",
    "    num_tfrecords = len(index) // num_samples\n",
    "    print('tfr directory: ', tfr_dir)\n",
    "    if len(index) % num_samples:\n",
    "        num_tfrecords += 1\n",
    "    if not os.path.exists(tfr_dir):\n",
    "        os.makedirs(tfr_dir)\n",
    "        os.makedirs(tfr_dir+'/tfrecords')\n",
    "    tfr_file_dir = tfr_dir+'/tfrecords'\n",
    "    tf_opts = tf.io.TFRecordOptions(compression_type='ZLIB')\n",
    "    for tfrec_num in tqdm(range(num_tfrecords)):\n",
    "        end = ((tfrec_num + 1) * num_samples)\n",
    "        end = end if end < len(index) else len(index)-1\n",
    "        idx_range = range((tfrec_num * num_samples) , end)\n",
    "\n",
    "        with tf.io.TFRecordWriter(\n",
    "            tfr_file_dir+'/'+dataset+ \"-%d.tfr\" % tfrec_num, tf_opts\n",
    "        ) as writer:\n",
    "            for idx in idx_range:\n",
    "                if include_seq:\n",
    "                    features_dict = {\n",
    "                    'sequence': feature_bytes(sub_embed[idx,:,:].astype('float16')),\n",
    "                    'target': feature_bytes(sub_target[idx].astype('float16')),\n",
    "                    'onehot' : feature_bytes(sub_seq[idx,:,:].astype('float16'))\n",
    "                    }\n",
    "                else:\n",
    "                    features_dict = {\n",
    "                    'sequence': feature_bytes(sub_embed[idx,:,:].astype('float16')),\n",
    "                    'target': feature_bytes(sub_target[idx].astype('float16'))\n",
    "                    }\n",
    "                example = tf.train.Example(features=tf.train.Features(feature=features_dict))\n",
    "                writer.write(example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_dict = {}\n",
    "stats_dict['num_targets'] = 1\n",
    "stats_dict ['onehot_length']=seq_onehot.shape[1]\n",
    "stats_dict['embed_length'] =lenti_embed.shape[1]\n",
    "stats_dict['embed_dim'] =lenti_embed.shape[2]\n",
    "stats_dict['crop_bp'] = 0\n",
    "stats_dict['train_seqs'] = len(train_index)\n",
    "stats_dict['valid_seqs'] =len(valid_index)\n",
    "stats_dict['test_seqs'] = len(test_index)\n",
    "\n",
    "with open('%s/statistics.json' % tfr_dir, 'w') as stats_json_out:\n",
    "  json.dump(stats_dict, stats_json_out, indent=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Move to tfr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-28 14:04:30.691429: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-28 14:04:31.265577: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import json\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "\n",
    "tfr_dir = '/home/ztang/multitask_RNA/data/lenti_MPRA_embed/HepG2_2B_1000G/'\n",
    "filtered_file = h5py.File('/home/ztang/multitask_RNA/data/lenti_MPRA/HepG2_data_embed.h5','r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_bytes(values):\n",
    "  \"\"\"Convert numpy arrays to bytes features.\"\"\"\n",
    "  values = values.flatten().tostring()\n",
    "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[values]))\n",
    "\n",
    "def feature_str(values):\n",
    "  \"\"\"Convert str to bytes features.\"\"\"\n",
    "  # value = np.array(values)\n",
    "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[values]))\n",
    "\n",
    "def feature_floats(values):\n",
    "  \"\"\"Convert numpy arrays to floats features.\n",
    "     Requires more space than bytes.\"\"\"\n",
    "  values = values.flatten().tolist()\n",
    "  return tf.train.Feature(float_list=tf.train.FloatList(value=values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h5_to_tfr(dataset,filtered_file,tfr_dir):\n",
    "    num_samples = 512\n",
    "    num_tfrecords = len(filtered_file[dataset+'_x']) // num_samples\n",
    "    print('dataset size: '+ str(len(filtered_file[dataset+'_x'])))\n",
    "    if len(filtered_file[dataset+'_x']) % num_samples:\n",
    "        num_tfrecords += 1\n",
    "    if not os.path.exists(tfr_dir):\n",
    "        os.makedirs(tfr_dir)\n",
    "        os.makedirs(tfr_dir+'/tfrecords')\n",
    "    tfr_file_dir = tfr_dir+'/tfrecords'\n",
    "    tf_opts = tf.io.TFRecordOptions(compression_type='ZLIB')\n",
    "    for tfrec_num in tqdm(range(num_tfrecords)):\n",
    "        end = ((tfrec_num + 1) * num_samples)\n",
    "        end = end if end < len(filtered_file[dataset+'_x']) else len(filtered_file[dataset+'_x'])-1\n",
    "        idx_range = range((tfrec_num * num_samples) , end)\n",
    "\n",
    "        with tf.io.TFRecordWriter(\n",
    "            tfr_file_dir+'/'+dataset+ \"-%d.tfr\" % tfrec_num, tf_opts\n",
    "        ) as writer:\n",
    "            for idx in idx_range:\n",
    "                features_dict = {\n",
    "                'sequence': feature_bytes(filtered_file[dataset+'_x'][idx,:,:].astype('float16')),\n",
    "                'target': feature_bytes(filtered_file[dataset+'_y'][idx].astype('float16'))\n",
    "                }\n",
    "                example = tf.train.Example(features=tf.train.Features(feature=features_dict))\n",
    "                writer.write(example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size: 13988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/28 [00:00<?, ?it/s]/tmp/ipykernel_651069/3740337950.py:3: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  values = values.flatten().tostring()\n",
      "  0%|          | 0/28 [00:22<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m h5_to_tfr(\u001b[39m'\u001b[39;49m\u001b[39mtest\u001b[39;49m\u001b[39m'\u001b[39;49m,filtered_file,tfr_dir)\n",
      "Cell \u001b[0;32mIn[3], line 22\u001b[0m, in \u001b[0;36mh5_to_tfr\u001b[0;34m(dataset, filtered_file, tfr_dir)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mTFRecordWriter(\n\u001b[1;32m     18\u001b[0m     tfr_file_dir\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39mdataset\u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m-\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m.tfr\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m tfrec_num, tf_opts\n\u001b[1;32m     19\u001b[0m ) \u001b[39mas\u001b[39;00m writer:\n\u001b[1;32m     20\u001b[0m     \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m idx_range:\n\u001b[1;32m     21\u001b[0m         features_dict \u001b[39m=\u001b[39m {\n\u001b[0;32m---> 22\u001b[0m         \u001b[39m'\u001b[39m\u001b[39msequence\u001b[39m\u001b[39m'\u001b[39m: feature_bytes(filtered_file[dataset\u001b[39m+\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m_x\u001b[39;49m\u001b[39m'\u001b[39;49m][idx,:,:]\u001b[39m.\u001b[39mastype(\u001b[39m'\u001b[39m\u001b[39mfloat16\u001b[39m\u001b[39m'\u001b[39m)),\n\u001b[1;32m     23\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mtarget\u001b[39m\u001b[39m'\u001b[39m: feature_bytes(filtered_file[dataset\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m_y\u001b[39m\u001b[39m'\u001b[39m][idx]\u001b[39m.\u001b[39mastype(\u001b[39m'\u001b[39m\u001b[39mfloat16\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m     24\u001b[0m         }\n\u001b[1;32m     25\u001b[0m         example \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mtrain\u001b[39m.\u001b[39mExample(features\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mtrain\u001b[39m.\u001b[39mFeatures(feature\u001b[39m=\u001b[39mfeatures_dict))\n\u001b[1;32m     26\u001b[0m         writer\u001b[39m.\u001b[39mwrite(example\u001b[39m.\u001b[39mSerializeToString())\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/tf_torch/lib/python3.10/site-packages/h5py/_hl/dataset.py:768\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, args, new_dtype)\u001b[0m\n\u001b[1;32m    766\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fast_read_ok \u001b[39mand\u001b[39;00m (new_dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    767\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fast_reader\u001b[39m.\u001b[39;49mread(args)\n\u001b[1;32m    769\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    770\u001b[0m         \u001b[39mpass\u001b[39;00m  \u001b[39m# Fall back to Python read pathway below\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "h5_to_tfr('test',filtered_file,tfr_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_dict = {}\n",
    "stats_dict['num_targets'] = filtered_file['train_y'].shape[-1]\n",
    "stats_dict['seq_length'] = filtered_file['train_x'].shape[1]\n",
    "stats_dict['pool_width'] = 1\n",
    "stats_dict['crop_bp'] = 0\n",
    "stats_dict['target_length'] = filtered_file['train_y'].shape[1]\n",
    "stats_dict['train_seqs'] = filtered_file['train_x'].shape[0]\n",
    "stats_dict['valid_seqs'] = filtered_file['valid_x'].shape[0]\n",
    "\n",
    "with open('%s/statistics.json' % tfr_dir, 'w') as stats_json_out:\n",
    "  json.dump(stats_dict, stats_json_out, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
