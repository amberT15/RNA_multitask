{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sequence_models\n",
    "from sequence_models.constants import SPECIALS\n",
    "from sequence_models.pretrained import load_carp\n",
    "from sequence_models.collaters import SimpleCollater\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='7'\n",
    "import sys\n",
    "sys.path.append('/home/amber/multitask_RNA/data_generation/')\n",
    "sys.path.append('/home/amber/multitask_RNA/rna_self_train/')\n",
    "import rna_model\n",
    "from torchinfo import summary\n",
    "import h5py\n",
    "import utils\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNA='ACGTN'\n",
    "RNA_ALPHABET = RNA+SPECIALS\n",
    "collater = SimpleCollater(RNA_ALPHABET,False,False)\n",
    "class SaveOutput:\n",
    "    def __init__(self):\n",
    "        self.outputs = []\n",
    "        \n",
    "    def __call__(self, module, module_in, module_out):\n",
    "        self.outputs.extend(module_out.cpu().detach().numpy())\n",
    "        \n",
    "    def clear(self):\n",
    "        self.outputs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ByteNetLM(\n",
       "  (embedder): ByteNet(\n",
       "    (embedder): Embedding(9, 9, padding_idx=6)\n",
       "    (up_embedder): PositionFeedForward(\n",
       "      (conv): Conv1d(9, 320, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): ByteNetBlock(\n",
       "        (conv): MaskedConv1d(160, 160, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        (sequence1): Sequential(\n",
       "          (0): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): ReLU()\n",
       "          (2): PositionFeedForward(\n",
       "            (conv): Conv1d(320, 160, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "          (3): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "          (4): ReLU()\n",
       "        )\n",
       "        (sequence2): Sequential(\n",
       "          (0): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): ReLU()\n",
       "          (2): PositionFeedForward(\n",
       "            (conv): Conv1d(160, 320, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): ByteNetBlock(\n",
       "        (conv): MaskedConv1d(160, 160, kernel_size=(5,), stride=(1,), padding=(4,), dilation=(2,))\n",
       "        (sequence1): Sequential(\n",
       "          (0): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): ReLU()\n",
       "          (2): PositionFeedForward(\n",
       "            (conv): Conv1d(320, 160, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "          (3): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "          (4): ReLU()\n",
       "        )\n",
       "        (sequence2): Sequential(\n",
       "          (0): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): ReLU()\n",
       "          (2): PositionFeedForward(\n",
       "            (conv): Conv1d(160, 320, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): ByteNetBlock(\n",
       "        (conv): MaskedConv1d(160, 160, kernel_size=(5,), stride=(1,), padding=(8,), dilation=(4,))\n",
       "        (sequence1): Sequential(\n",
       "          (0): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): ReLU()\n",
       "          (2): PositionFeedForward(\n",
       "            (conv): Conv1d(320, 160, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "          (3): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "          (4): ReLU()\n",
       "        )\n",
       "        (sequence2): Sequential(\n",
       "          (0): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): ReLU()\n",
       "          (2): PositionFeedForward(\n",
       "            (conv): Conv1d(160, 320, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): ByteNetBlock(\n",
       "        (conv): MaskedConv1d(160, 160, kernel_size=(5,), stride=(1,), padding=(16,), dilation=(8,))\n",
       "        (sequence1): Sequential(\n",
       "          (0): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): ReLU()\n",
       "          (2): PositionFeedForward(\n",
       "            (conv): Conv1d(320, 160, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "          (3): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "          (4): ReLU()\n",
       "        )\n",
       "        (sequence2): Sequential(\n",
       "          (0): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): ReLU()\n",
       "          (2): PositionFeedForward(\n",
       "            (conv): Conv1d(160, 320, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): ByteNetBlock(\n",
       "        (conv): MaskedConv1d(160, 160, kernel_size=(5,), stride=(1,), padding=(32,), dilation=(16,))\n",
       "        (sequence1): Sequential(\n",
       "          (0): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): ReLU()\n",
       "          (2): PositionFeedForward(\n",
       "            (conv): Conv1d(320, 160, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "          (3): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "          (4): ReLU()\n",
       "        )\n",
       "        (sequence2): Sequential(\n",
       "          (0): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): ReLU()\n",
       "          (2): PositionFeedForward(\n",
       "            (conv): Conv1d(160, 320, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): ByteNetBlock(\n",
       "        (conv): MaskedConv1d(160, 160, kernel_size=(5,), stride=(1,), padding=(64,), dilation=(32,))\n",
       "        (sequence1): Sequential(\n",
       "          (0): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): ReLU()\n",
       "          (2): PositionFeedForward(\n",
       "            (conv): Conv1d(320, 160, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "          (3): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "          (4): ReLU()\n",
       "        )\n",
       "        (sequence2): Sequential(\n",
       "          (0): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): ReLU()\n",
       "          (2): PositionFeedForward(\n",
       "            (conv): Conv1d(160, 320, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): ByteNetBlock(\n",
       "        (conv): MaskedConv1d(160, 160, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        (sequence1): Sequential(\n",
       "          (0): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): ReLU()\n",
       "          (2): PositionFeedForward(\n",
       "            (conv): Conv1d(320, 160, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "          (3): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "          (4): ReLU()\n",
       "        )\n",
       "        (sequence2): Sequential(\n",
       "          (0): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): ReLU()\n",
       "          (2): PositionFeedForward(\n",
       "            (conv): Conv1d(160, 320, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): ByteNetBlock(\n",
       "        (conv): MaskedConv1d(160, 160, kernel_size=(5,), stride=(1,), padding=(4,), dilation=(2,))\n",
       "        (sequence1): Sequential(\n",
       "          (0): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): ReLU()\n",
       "          (2): PositionFeedForward(\n",
       "            (conv): Conv1d(320, 160, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "          (3): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "          (4): ReLU()\n",
       "        )\n",
       "        (sequence2): Sequential(\n",
       "          (0): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): ReLU()\n",
       "          (2): PositionFeedForward(\n",
       "            (conv): Conv1d(160, 320, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): ByteNetBlock(\n",
       "        (conv): MaskedConv1d(160, 160, kernel_size=(5,), stride=(1,), padding=(8,), dilation=(4,))\n",
       "        (sequence1): Sequential(\n",
       "          (0): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): ReLU()\n",
       "          (2): PositionFeedForward(\n",
       "            (conv): Conv1d(320, 160, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "          (3): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "          (4): ReLU()\n",
       "        )\n",
       "        (sequence2): Sequential(\n",
       "          (0): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): ReLU()\n",
       "          (2): PositionFeedForward(\n",
       "            (conv): Conv1d(160, 320, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): ByteNetBlock(\n",
       "        (conv): MaskedConv1d(160, 160, kernel_size=(5,), stride=(1,), padding=(16,), dilation=(8,))\n",
       "        (sequence1): Sequential(\n",
       "          (0): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): ReLU()\n",
       "          (2): PositionFeedForward(\n",
       "            (conv): Conv1d(320, 160, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "          (3): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "          (4): ReLU()\n",
       "        )\n",
       "        (sequence2): Sequential(\n",
       "          (0): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): ReLU()\n",
       "          (2): PositionFeedForward(\n",
       "            (conv): Conv1d(160, 320, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (10): ByteNetBlock(\n",
       "        (conv): MaskedConv1d(160, 160, kernel_size=(5,), stride=(1,), padding=(32,), dilation=(16,))\n",
       "        (sequence1): Sequential(\n",
       "          (0): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): ReLU()\n",
       "          (2): PositionFeedForward(\n",
       "            (conv): Conv1d(320, 160, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "          (3): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "          (4): ReLU()\n",
       "        )\n",
       "        (sequence2): Sequential(\n",
       "          (0): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): ReLU()\n",
       "          (2): PositionFeedForward(\n",
       "            (conv): Conv1d(160, 320, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): ByteNetBlock(\n",
       "        (conv): MaskedConv1d(160, 160, kernel_size=(5,), stride=(1,), padding=(64,), dilation=(32,))\n",
       "        (sequence1): Sequential(\n",
       "          (0): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): ReLU()\n",
       "          (2): PositionFeedForward(\n",
       "            (conv): Conv1d(320, 160, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "          (3): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "          (4): ReLU()\n",
       "        )\n",
       "        (sequence2): Sequential(\n",
       "          (0): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): ReLU()\n",
       "          (2): PositionFeedForward(\n",
       "            (conv): Conv1d(160, 320, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (12): ByteNetBlock(\n",
       "        (conv): MaskedConv1d(160, 160, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        (sequence1): Sequential(\n",
       "          (0): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): ReLU()\n",
       "          (2): PositionFeedForward(\n",
       "            (conv): Conv1d(320, 160, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "          (3): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "          (4): ReLU()\n",
       "        )\n",
       "        (sequence2): Sequential(\n",
       "          (0): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): ReLU()\n",
       "          (2): PositionFeedForward(\n",
       "            (conv): Conv1d(160, 320, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (13): ByteNetBlock(\n",
       "        (conv): MaskedConv1d(160, 160, kernel_size=(5,), stride=(1,), padding=(4,), dilation=(2,))\n",
       "        (sequence1): Sequential(\n",
       "          (0): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): ReLU()\n",
       "          (2): PositionFeedForward(\n",
       "            (conv): Conv1d(320, 160, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "          (3): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "          (4): ReLU()\n",
       "        )\n",
       "        (sequence2): Sequential(\n",
       "          (0): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): ReLU()\n",
       "          (2): PositionFeedForward(\n",
       "            (conv): Conv1d(160, 320, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (14): ByteNetBlock(\n",
       "        (conv): MaskedConv1d(160, 160, kernel_size=(5,), stride=(1,), padding=(8,), dilation=(4,))\n",
       "        (sequence1): Sequential(\n",
       "          (0): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): ReLU()\n",
       "          (2): PositionFeedForward(\n",
       "            (conv): Conv1d(320, 160, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "          (3): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "          (4): ReLU()\n",
       "        )\n",
       "        (sequence2): Sequential(\n",
       "          (0): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): ReLU()\n",
       "          (2): PositionFeedForward(\n",
       "            (conv): Conv1d(160, 320, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): PositionFeedForward(\n",
       "    (conv): Conv1d(320, 9, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       "  (last_norm): Identity()\n",
       "  (loss_func): MaskedCrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config={'model':'ByteNetLM',\n",
    "                'lr':1e-3,\n",
    "                'n_tokens':len(RNA_ALPHABET),\n",
    "                'd_embedding' : 9, # dimension of embedding\n",
    "                'd_model': 320, # dimension to use within ByteNet model, //2 every layer\n",
    "                'n_layers' : 15, # number of layers of ByteNet block\n",
    "                'activation': 'relu',\n",
    "                'kernel_size' : 5, # the kernel width\n",
    "                'r' : 32, # used to calculate dilation factor\n",
    "                'padding_idx' : RNA_ALPHABET.index('-') ,# location of padding token in ordered alphabet\n",
    "                'dropout' : 0.1 ,\n",
    "                }\n",
    "\n",
    "model = rna_model.ByteNetLM(config['n_tokens'], config['d_embedding'], config['d_model'],\n",
    "                        config['n_layers'], config['kernel_size'], config['r'], config['lr'],\n",
    "                        padding_idx=config['padding_idx'], causal=False, dropout=config['dropout'])\n",
    "model.load_state_dict(torch.load('/home/amber/multitask_RNA/rna_self_train/rna-selftrain/2hkapjgg/checkpoints/best_model.ckpt')['state_dict'])\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_output = SaveOutput()\n",
    "hook_handles = []\n",
    "handle = model.last_norm.register_forward_hook(save_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/18 [00:00<?, ?it/s]/tmp/ipykernel_4009550/375517989.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  seq_batch = torch.tensor(token_output[seq_i:seq_i+batch_size]).to(device)\n",
      "100%|██████████| 18/18 [00:00<00:00, 23.40it/s]\n",
      "100%|██████████| 143/143 [00:05<00:00, 26.48it/s]\n",
      "100%|██████████| 18/18 [00:00<00:00, 33.76it/s]\n"
     ]
    }
   ],
   "source": [
    "file = h5py.File('../../data/rna_stable/insert_dataset.h5','r')\n",
    "carp_output = h5py.File('../../data/rna_stable/carp_embed.h5','w')\n",
    "batch_size = 64\n",
    "for dataset in ['test','train','valid']:\n",
    "    key = 'X_'+dataset\n",
    "    onehot = file[key]\n",
    "    string_seq = utils.onehot_to_seq(onehot)\n",
    "    expand_seq = np.expand_dims(np.array(string_seq),axis = -1)\n",
    "    token_output = collater(expand_seq)[0]\n",
    "\n",
    "    save_output = SaveOutput()\n",
    "    handle = model.last_norm.register_forward_hook(save_output)\n",
    "\n",
    "    for seq_i in tqdm(range(0,len(token_output),batch_size)):\n",
    "        seq_batch = torch.tensor(token_output[seq_i:seq_i+batch_size]).to(device)\n",
    "        output_seq = model(seq_batch).cpu().detach().numpy()\n",
    "    carp_output.create_dataset(name=key,data = np.array(save_output.outputs))\n",
    "    carp_output.create_dataset(name='Y_'+dataset,data = file['Y_'+dataset][:])\n",
    "    handle.remove()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"X_train\": shape (9131, 173, 320), type \"<f4\">"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "carp_output['X_train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "carp_output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('tf_2': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9e8284e1417b754e460c2bde3a4a4837c482fa82ceb7d52f4acbe340dd4b4559"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
