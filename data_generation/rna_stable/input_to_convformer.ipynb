{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append('/home/amber/multitask_RNA/rna_self_train/')\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='7'\n",
    "import rna_model\n",
    "import h5py\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = {\n",
    "    'model':'conv_former',\n",
    "    'hidden_size': 512,\n",
    "    'attention_window': [256,256,256],\n",
    "    'num_attention_heads': 8,\n",
    "    'intermediate_size': 2048,\n",
    "    'attention_dilation': [1,1,1],\n",
    "    'data_dir' : '/grid/koo/home/ztang/multitask_RNA/data/pre-train/3072/rna_onehot.h5',\n",
    "    'batch_size':16}\n",
    "ckpt_path = '/home/amber/multitask_RNA/rna_self_train/model_ckpt/convformer_linear_decay/epoch=73-step=23200.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "conv_former(\n",
       "  (conv1): Conv1d(4, 512, kernel_size=(19,), stride=(1,), padding=same)\n",
       "  (batchnorm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (block1): dilated_residual(\n",
       "    (block): Sequential(\n",
       "      (0): Conv1d(512, 512, kernel_size=(7,), stride=(1,), padding=same)\n",
       "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Dropout(p=0.1, inplace=False)\n",
       "      (4): Conv1d(512, 512, kernel_size=(7,), stride=(1,), padding=same, dilation=(2,))\n",
       "      (5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (output_act): ReLU()\n",
       "  )\n",
       "  (block2): dilated_residual(\n",
       "    (block): Sequential(\n",
       "      (0): Conv1d(512, 512, kernel_size=(7,), stride=(1,), padding=same)\n",
       "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Dropout(p=0.1, inplace=False)\n",
       "      (4): Conv1d(512, 512, kernel_size=(7,), stride=(1,), padding=same, dilation=(4,))\n",
       "      (5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (output_act): ReLU()\n",
       "  )\n",
       "  (att_list): ModuleList(\n",
       "    (0): longformer_block(\n",
       "      (attention): LongformerSelfAttention(\n",
       "        (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (query_global): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (key_global): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value_global): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (linear_project): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layernorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "      (ff1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "      (activation): GELU()\n",
       "      (ff2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      (drop2): Dropout(p=0.1, inplace=False)\n",
       "      (layernorm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (1): longformer_block(\n",
       "      (attention): LongformerSelfAttention(\n",
       "        (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (query_global): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (key_global): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value_global): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (linear_project): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layernorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "      (ff1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "      (activation): GELU()\n",
       "      (ff2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      (drop2): Dropout(p=0.1, inplace=False)\n",
       "      (layernorm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (2): longformer_block(\n",
       "      (attention): LongformerSelfAttention(\n",
       "        (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (query_global): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (key_global): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value_global): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (linear_project): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layernorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "      (ff1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "      (activation): GELU()\n",
       "      (ff2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      (drop2): Dropout(p=0.1, inplace=False)\n",
       "      (layernorm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (output_head): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (1): GELU()\n",
       "    (2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (vocab_linear): Linear(in_features=512, out_features=4, bias=True)\n",
       "  (loss_func): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = rna_model.conv_former_config(**config_dict)\n",
    "model = rna_model.conv_former.load_from_checkpoint(ckpt_path,config = config).cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x7f52d23c9e20>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model,input,output):\n",
    "        if name not in activation.keys():\n",
    "            activation[name] = []\n",
    "        activation[name].extend(output[0].cpu().detach().numpy())\n",
    "    return hook\n",
    "\n",
    "model.att_list[-1].register_forward_hook(get_activation('att'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "(1137, 4, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/72 [00:00<?, ?it/s]/tmp/ipykernel_1365600/3172365481.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  output = model(torch.tensor(seq_batch))\n",
      "100%|██████████| 72/72 [00:03<00:00, 21.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1137, 173, 512)\n",
      "train\n",
      "(9131, 4, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 571/571 [00:26<00:00, 21.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9131, 173, 512)\n",
      "valid\n",
      "(1149, 4, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72/72 [00:03<00:00, 23.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1149, 173, 512)\n"
     ]
    }
   ],
   "source": [
    "file = h5py.File('../../data/rna_stable/insert_dataset.h5','r')\n",
    "convformer_output = h5py.File('../../data/rna_stable/convformer_output.h5','w')\n",
    "batch_size = 16\n",
    "for dataset in ['test','train','valid']:\n",
    "    print(dataset)\n",
    "    key = 'X_'+dataset\n",
    "    onehot = np.transpose(file[key][()],(0,2,1))\n",
    "    pad_onehot = np.pad(onehot,((0,0),(0,0),(169,170)), mode='constant', constant_values=0)\n",
    "    print(pad_onehot.shape)\n",
    "\n",
    "    for seq_i in tqdm(range(0,len(pad_onehot),batch_size)):\n",
    "        seq_batch = torch.tensor(pad_onehot[seq_i:seq_i+batch_size]).to('cuda')\n",
    "        output = model(torch.tensor(seq_batch))\n",
    "        \n",
    "    convformer_rep = np.array(activation['att'])[:,169:-170,:]\n",
    "    print(convformer_rep.shape)\n",
    "    convformer_output.create_dataset(key, data=convformer_rep)\n",
    "    convformer_output.create_dataset(name='Y_'+dataset,data = file['Y_'+dataset][:])\n",
    "    activation = {}\n",
    "\n",
    "convformer_output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpn_file = h5py.File('../../data/rna_stable/gpn_finetune_embed.h5','r')\n",
    "conv_file = h5py.File('../../data/rna_stable/convformer_embed.h5','r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['X_test', 'X_train', 'X_valid', 'Y_test', 'Y_train', 'Y_valid']>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpn_file.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['X_test', 'X_train', 'X_valid', 'Y_test', 'Y_train', 'Y_valid']>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_file.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"X_test\": shape (1137, 173, 512), type \"<f4\">"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_file['X_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9e8284e1417b754e460c2bde3a4a4837c482fa82ceb7d52f4acbe340dd4b4559"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
