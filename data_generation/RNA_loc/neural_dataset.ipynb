{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f496aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import RobertaForMaskedLM\n",
    "sys.path.append('/home/amber/multitask_RNA/DNA_BERT_rep/')\n",
    "from dna_tokenizer import DNATokenizer\n",
    "os.environ['CUDA_VISIBLE_DEVICES']= '7'\n",
    "def onehotcoding(seq):\n",
    "    IN_MAP = np.asarray([[0, 0, 0, 0],\n",
    "                     [1, 0, 0, 0],\n",
    "                     [0, 1, 0, 0],\n",
    "                     [0, 0, 1, 0],\n",
    "                     [0, 0, 0, 1]])\n",
    "    seq = seq.upper().replace('A', '1').replace('C', '2')\n",
    "    seq = seq.replace('G', '3').replace('T', '4').replace('N', '0')\n",
    "    return IN_MAP[np.array(list(seq)).astype(int)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35d6f673",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RobertaForMaskedLM.from_pretrained('/home/amber/multitask_RNA/DNA_BERT_rep/small-roberta-lr8/checkpoint-23500/')\n",
    "tokenizer = DNATokenizer('/home/amber/multitask_RNA/DNA_BERT_rep/vocab.txt')\n",
    "rna_local = pd.read_csv('../data/RNA_loc/GSE173098_RNAloc_MPRA_counts.csv',index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5bc0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('cuda:0')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "208bba20",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/amber/multitask_RNA/data_generation/neural_dataset.ipynb Cell 4\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcitra/home/amber/multitask_RNA/data_generation/neural_dataset.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m seq \u001b[39m=\u001b[39m row[\u001b[39m'\u001b[39m\u001b[39msynthetic library sequence\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcitra/home/amber/multitask_RNA/data_generation/neural_dataset.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m vector \u001b[39m=\u001b[39m row\u001b[39m.\u001b[39mvalues[\u001b[39m1\u001b[39m:]\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bcitra/home/amber/multitask_RNA/data_generation/neural_dataset.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m seq_token \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39;49mbatch_encode_plus(seq,return_tensors\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcitra/home/amber/multitask_RNA/data_generation/neural_dataset.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m                                 add_special_tokens\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcitra/home/amber/multitask_RNA/data_generation/neural_dataset.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m x\u001b[39m.\u001b[39mappend(seq)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcitra/home/amber/multitask_RNA/data_generation/neural_dataset.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m y\u001b[39m.\u001b[39mappend(np\u001b[39m.\u001b[39marray(vector)\u001b[39m.\u001b[39mastype(\u001b[39mint\u001b[39m))\n",
      "File \u001b[0;32m~/multitask_RNA/DNA_BERT_rep/dna_tokenizer.py:597\u001b[0m, in \u001b[0;36mDNATokenizer.batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, max_length, stride, truncation_strategy, return_tensors, return_input_lengths, return_attention_masks, return_offsets_mapping, context_split, **kwargs)\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    596\u001b[0m     ids, pair_ids \u001b[39m=\u001b[39m ids_or_pair_ids, \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 597\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencode_plus(\n\u001b[1;32m    598\u001b[0m     ids,\n\u001b[1;32m    599\u001b[0m     pair_ids,\n\u001b[1;32m    600\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m    601\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m    602\u001b[0m     stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m    603\u001b[0m     truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[1;32m    604\u001b[0m     return_tensors\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    605\u001b[0m )\n\u001b[1;32m    607\u001b[0m \u001b[39m# Append the non-padded length to the output\u001b[39;00m\n\u001b[1;32m    608\u001b[0m \u001b[39mif\u001b[39;00m return_input_lengths:\n",
      "File \u001b[0;32m~/multitask_RNA/DNA_BERT_rep/dna_tokenizer.py:219\u001b[0m, in \u001b[0;36mDNATokenizer.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, max_length, stride, truncation, padding, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, **kwargs)\u001b[0m\n\u001b[1;32m    217\u001b[0m context_split \u001b[39m=\u001b[39m  kwargs\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mcontext_split\u001b[39m\u001b[39m'\u001b[39m,\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    218\u001b[0m \u001b[39mif\u001b[39;00m context_split \u001b[39m==\u001b[39m \u001b[39mFalse\u001b[39;00m:\n\u001b[0;32m--> 219\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprepare_for_model(\n\u001b[1;32m    220\u001b[0m         first_ids,\n\u001b[1;32m    221\u001b[0m         pair_ids\u001b[39m=\u001b[39;49msecond_ids,\n\u001b[1;32m    222\u001b[0m         max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m    223\u001b[0m         pad_to_max_length\u001b[39m=\u001b[39;49mpadding,\n\u001b[1;32m    224\u001b[0m         add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m    225\u001b[0m         stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m    226\u001b[0m         truncation_strategy\u001b[39m=\u001b[39;49mtruncation,\n\u001b[1;32m    227\u001b[0m         return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m    228\u001b[0m         return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m    229\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m    230\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m    231\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m    232\u001b[0m     )\n\u001b[1;32m    233\u001b[0m \u001b[39melif\u001b[39;00m context_split \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    234\u001b[0m     pair \u001b[39m=\u001b[39m( second_ids \u001b[39m!=\u001b[39m \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/multitask_RNA/DNA_BERT_rep/dna_tokenizer.py:444\u001b[0m, in \u001b[0;36mDNATokenizer.prepare_for_model\u001b[0;34m(self, ids, pair_ids, max_length, add_special_tokens, stride, truncation_strategy, pad_to_max_length, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[39mif\u001b[39;00m add_special_tokens:\n\u001b[1;32m    443\u001b[0m     sequence \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuild_inputs_with_special_tokens(ids, pair_ids)\n\u001b[0;32m--> 444\u001b[0m     token_type_ids \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcreate_token_type_ids_from_sequences(ids, pair_ids)\n\u001b[1;32m    445\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    446\u001b[0m     sequence \u001b[39m=\u001b[39m ids \u001b[39m+\u001b[39m pair_ids \u001b[39mif\u001b[39;00m pair \u001b[39melse\u001b[39;00m ids\n",
      "File \u001b[0;32m~/multitask_RNA/DNA_BERT_rep/dna_tokenizer.py:760\u001b[0m, in \u001b[0;36mDNATokenizer.create_token_type_ids_from_sequences\u001b[0;34m(self, token_ids_0, token_ids_1)\u001b[0m\n\u001b[1;32m    751\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate_token_type_ids_from_sequences\u001b[39m(\u001b[39mself\u001b[39m, token_ids_0, token_ids_1\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    752\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    753\u001b[0m \u001b[39m    Creates a mask from the two sequences passed to be used in a sequence-pair classification task.\u001b[39;00m\n\u001b[1;32m    754\u001b[0m \u001b[39m    A BERT sequence pair mask has the following format:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    758\u001b[0m \u001b[39m    if token_ids_1 is None, only returns the first portion of the mask (0's).\u001b[39;00m\n\u001b[1;32m    759\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 760\u001b[0m     sep \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msep_token_id]\n\u001b[1;32m    761\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcls_token_id]\n\u001b[1;32m    762\u001b[0m     \u001b[39mif\u001b[39;00m token_ids_1 \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/multitask_RNA/DNA_BERT_rep/dna_tokenizer.py:792\u001b[0m, in \u001b[0;36mDNATokenizer.sep_token_id\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m    790\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msep_token_id\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    791\u001b[0m     \u001b[39m\"\"\" Id of the separation token in the vocabulary. E.g. separate context and query in an input sequence. Log an error if used while not having been set. \"\"\"\u001b[39;00m\n\u001b[0;32m--> 792\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_tokens_to_ids(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msep_token)\n",
      "File \u001b[0;32m~/tf_2/lib/python3.8/site-packages/transformers/tokenization_utils.py:575\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.convert_tokens_to_ids\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    572\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    574\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(tokens, \u001b[39mstr\u001b[39m):\n\u001b[0;32m--> 575\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_convert_token_to_id_with_added_voc(tokens)\n\u001b[1;32m    577\u001b[0m ids \u001b[39m=\u001b[39m []\n\u001b[1;32m    578\u001b[0m \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m tokens:\n",
      "File \u001b[0;32m~/tf_2/lib/python3.8/site-packages/transformers/tokenization_utils.py:588\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._convert_token_to_id_with_added_voc\u001b[0;34m(self, token)\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[39mif\u001b[39;00m token \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madded_tokens_encoder:\n\u001b[1;32m    587\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madded_tokens_encoder[token]\n\u001b[0;32m--> 588\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_convert_token_to_id(token)\n",
      "File \u001b[0;32m~/multitask_RNA/DNA_BERT_rep/dna_tokenizer.py:167\u001b[0m, in \u001b[0;36mDNATokenizer._convert_token_to_id\u001b[0;34m(self, token)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_convert_token_to_id\u001b[39m(\u001b[39mself\u001b[39m, token):\n\u001b[1;32m    166\u001b[0m     \u001b[39m\"\"\"Converts a token (str) in an id using the vocab.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 167\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab\u001b[39m.\u001b[39mget(token, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab\u001b[39m.\u001b[39mget(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49munk_token))\n",
      "File \u001b[0;32m~/tf_2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:974\u001b[0m, in \u001b[0;36mSpecialTokensMixin.unk_token\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    971\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    972\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eos_token)\n\u001b[0;32m--> 974\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m    975\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39munk_token\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m    976\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    977\u001b[0m \u001b[39m    `str`: Unknown token. Log an error if used while not having been set.\u001b[39;00m\n\u001b[1;32m    978\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m    979\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_unk_token \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x = []\n",
    "y = []\n",
    "for (i,row) in rna_local.iterrows():\n",
    "    seq = row['synthetic library sequence']\n",
    "    vector = row.values[1:]\n",
    "    \n",
    "    seq_token = tokenizer.batch_encode_plus(seq,return_tensors='pt',\n",
    "                                    add_special_tokens=True)\n",
    "    x.append(seq)\n",
    "    y.append(np.array(vector).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbbfa6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dab954ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "for (i,row) in rna_local.iterrows():\n",
    "    seq = row['synthetic library sequence']\n",
    "    vector = row.values[1:]\n",
    "    onehot = onehotcoding(seq)\n",
    "    x.append(onehot)\n",
    "    y.append(np.array(vector).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9578357d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object dtype dtype('O') has no native HDF5 equivalent",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [48]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m h5f \u001b[38;5;241m=\u001b[39m h5py\u001b[38;5;241m.\u001b[39mFile(file_name, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m x \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mcreate_dataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m,data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(x))\n\u001b[0;32m----> 5\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43my\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tf_2/lib/python3.8/site-packages/h5py/_hl/group.py:149\u001b[0m, in \u001b[0;36mGroup.create_dataset\u001b[0;34m(self, name, shape, dtype, data, **kwds)\u001b[0m\n\u001b[1;32m    146\u001b[0m         parent_path, name \u001b[38;5;241m=\u001b[39m name\u001b[38;5;241m.\u001b[39mrsplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    147\u001b[0m         group \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequire_group(parent_path)\n\u001b[0;32m--> 149\u001b[0m dsid \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_new_dset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m dset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mDataset(dsid)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dset\n",
      "File \u001b[0;32m~/tf_2/lib/python3.8/site-packages/h5py/_hl/dataset.py:91\u001b[0m, in \u001b[0;36mmake_new_dset\u001b[0;34m(parent, shape, dtype, data, name, chunks, compression, shuffle, fletcher32, maxshape, compression_opts, fillvalue, scaleoffset, track_times, external, track_order, dcpl, allow_unknown_filter)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     90\u001b[0m         dtype \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39mdtype(dtype)\n\u001b[0;32m---> 91\u001b[0m     tid \u001b[38;5;241m=\u001b[39m \u001b[43mh5t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpy_create\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# Legacy\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m((compression, shuffle, fletcher32, maxshape, scaleoffset)) \u001b[38;5;129;01mand\u001b[39;00m chunks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[0;32mh5py/h5t.pyx:1663\u001b[0m, in \u001b[0;36mh5py.h5t.py_create\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5t.pyx:1687\u001b[0m, in \u001b[0;36mh5py.h5t.py_create\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5t.pyx:1747\u001b[0m, in \u001b[0;36mh5py.h5t.py_create\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Object dtype dtype('O') has no native HDF5 equivalent"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "file_name = './data/neuron_local/RNAloc.h5'\n",
    "h5f = h5py.File(file_name, 'w')\n",
    "x = h5f.create_dataset('x',data = np.array(x))\n",
    "y = h5f.create_dataset('y',data =np.array(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceda43fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('tf_2': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "9e8284e1417b754e460c2bde3a4a4837c482fa82ceb7d52f4acbe340dd4b4559"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
