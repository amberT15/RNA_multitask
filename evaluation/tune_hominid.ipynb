{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8758d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py, time\n",
    "import click\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from filelock import FileLock\n",
    "from ray import tune\n",
    "from ray.tune.integration.keras import TuneReportCallback\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from ray.air.callbacks.wandb import WandbLoggerCallback\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from scipy.stats import spearmanr\n",
    "import wandb\n",
    "import yaml\n",
    "\n",
    "from hominid import model_zoo, utils\n",
    "\n",
    "# set seed for reproducibility\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389b03a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c98776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics\n",
    "\n",
    "def Spearman(y_true, y_pred):\n",
    "     return ( tf.py_function(spearmanr, [tf.cast(y_pred, tf.float32),\n",
    "                       tf.cast(y_true, tf.float32)], Tout = tf.float32) ) \n",
    "from keras import backend as K\n",
    "\n",
    "def pearson_r(y_true, y_pred):\n",
    "    # use smoothing for not resulting in NaN values\n",
    "    # pearson correlation coefficient\n",
    "    # https://github.com/WenYanger/Keras_Metrics\n",
    "    epsilon = 10e-5\n",
    "    x = y_true\n",
    "    y = y_pred\n",
    "    mx = K.mean(x)\n",
    "    my = K.mean(y)\n",
    "    xm, ym = x - mx, y - my\n",
    "    r_num = K.sum(xm * ym)\n",
    "    x_square_sum = K.sum(xm * xm)\n",
    "    y_square_sum = K.sum(ym * ym)\n",
    "    r_den = K.sqrt(x_square_sum * y_square_sum)\n",
    "    r = r_num / (r_den + epsilon)\n",
    "    return K.mean(r)         \n",
    "\n",
    "def hominid_pipeline(config):\n",
    "\n",
    "    # ==============================================================================\n",
    "    # Load dataset\n",
    "    # ==============================================================================\n",
    "\n",
    "    dataset_path = '/home/chandana/projects/hominid/data/hepg2.h5'\n",
    "    with h5py.File(dataset_path, \"r\") as f:\n",
    "        x_train = f[\"x_train\"][:]\n",
    "        y_train = f[\"y_train\"][:]\n",
    "\n",
    "        x_valid = f[\"x_valid\"][:]\n",
    "        y_valid = f[\"y_valid\"][:]\n",
    "\n",
    "        x_test = f[\"x_test\"][:]\n",
    "        y_test = f[\"y_test\"][:]\n",
    "\n",
    "    N, L, A = x_train.shape\n",
    "    output_shape = y_train.shape[-1]\n",
    "\n",
    "    print(f\"Input shape: {N, L, A}. Output shape: {output_shape}\")\n",
    "\n",
    "    config[\"input_shape\"] = (L, A)\n",
    "    config[\"output_shape\"] = output_shape\n",
    "\n",
    "    print(output_shape)\n",
    "\n",
    "    # ==============================================================================\n",
    "    # Build model\n",
    "    # ==============================================================================\n",
    "\n",
    "    print(\"Building model...\")\n",
    "\n",
    "    model = model_zoo.base_model(**config)\n",
    "\n",
    "    return x_train, y_train, x_valid, y_valid, x_test, y_test, model\n",
    "\n",
    "def tune_hominid(config: dict):\n",
    "\n",
    "    x_train, y_train, x_valid, y_valid, x_test, y_test, model = hominid_pipeline(config)\n",
    "\n",
    "    model.compile(\n",
    "        tf.keras.optimizers.Adam(lr=0.001),\n",
    "        loss='mse',\n",
    "        metrics=[Spearman, pearson_r] #PearsonCorrelation()\n",
    "        )\n",
    "    model.summary()\n",
    "\n",
    "    # train model\n",
    "    model.fit(\n",
    "          x_train, y_train,\n",
    "          epochs=60,\n",
    "          batch_size=128,\n",
    "          shuffle=True,\n",
    "          validation_data=(x_valid, y_valid),\n",
    "          callbacks=[TuneReportCallback({\n",
    "              \"pearson_r\": \"pearson_r\"\n",
    "          })]\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fe199f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"conv1_activation\": tune.choice([\"exponential\", \"relu\"]), # relu\n",
    "    \"conv1_batchnorm\": tune.choice([True, False]), \n",
    "    \"conv1_channel_weight\": tune.choice([\"softconv\", \"se\"]),\n",
    "    \"conv1_dropout\": 0.2,\n",
    "    \"conv1_filters\": tune.choice([128, 192, 256, 512]), # go to 256, 512\n",
    "    \"conv1_kernel_size\": tune.choice([11, 15, 19]), # go to 11 15 19\n",
    "    \"conv1_max_pool\": 10,\n",
    "    \"conv1_pool_type\": \"attention\",\n",
    "    \"conv1_type\": \"pw\",\n",
    "    \"dense_activation\": \"relu\",\n",
    "    \"dense_batchnorm\": True,\n",
    "    \"dense_dropout\": tune.choice([[0.3, 0.3], [0.4, 0.4], [0.5, 0.5]]), # go to 0.30.3 to 0.5 0.5\n",
    "    \"dense_units\": tune.choice([[128, 256],[512, 256], [256, 256], [512, 512],[512, 1024]]), # use [512, 256] (explore values in between as well + higher)\n",
    "    \"input_shape\": None,\n",
    "    \"mha_d_model\": tune.choice([96, 192]),\n",
    "    \"mha_dropout\": 0.1,\n",
    "    \"mha_head_type\": \"pool\",\n",
    "    \"mha_heads\": tune.choice([4, 8]),\n",
    "    \"mha_layernorm\": False,\n",
    "    \"mha_pool_type\": \"attention\",\n",
    "    \"output_activation\": \"linear\",\n",
    "    \"output_shape\": None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058d639a",
   "metadata": {},
   "outputs": [],
   "source": [
    "smoke_test = False  # For testing purposes: set this to False to run the full experiment\n",
    "analysis = tune.run(\n",
    "    tune_hominid,\n",
    "    name=\"tune_hominid\",\n",
    "    callbacks=[WandbLoggerCallback(project=\"raytune-expanded-v1\")],\n",
    "    scheduler=AsyncHyperBandScheduler(\n",
    "        time_attr=\"training_iteration\",\n",
    "        max_t=400,\n",
    "        grace_period=20\n",
    "    ),\n",
    "    metric=\"pearson_r\",\n",
    "    mode=\"max\",\n",
    "    stop={\n",
    "        \"pearson_r\": 0.9,\n",
    "        \"training_iteration\": 5 if smoke_test else 100\n",
    "    },\n",
    "    num_samples=2 if smoke_test else 50,\n",
    "    resources_per_trial={\n",
    "        \"cpu\": 4,\n",
    "        \"gpu\": 1\n",
    "    },\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31e31fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best hyperparameters found were: \", analysis.best_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cd990e",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_DIR = '/home/chandana/projects/hominid/results/'\n",
    "print(\"Best hyperparameters found were: \", analysis.best_config)\n",
    "\n",
    "pd.DataFrame.from_dict(\n",
    "        analysis.best_config,\n",
    "        orient='index'\n",
    ").to_csv(\n",
    "        RESULTS_DIR + 'tune_best.csv',\n",
    "        header=True,\n",
    "        index=True\n",
    "        )\n",
    "analysis.results_df.to_csv(RESULTS_DIR + 'tune_all.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
