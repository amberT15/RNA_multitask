{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import sys\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "sys.path.append('/home/amber/multitask_RNA/data_generation')\n",
    "import utils\n",
    "import numpy as np\n",
    "import nucleotide_transformer\n",
    "import haiku as hk\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from nucleotide_transformer.pretrained import get_pretrained_model\n",
    "from tqdm import tqdm\n",
    "model_name = '500M_1000G'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = h5py.File('/home/amber/multitask_RNA/data/lenti_MPRA/WTC11_data.h5', 'r')\n",
    "sequence = data_file['seq'][()]\n",
    "target = data_file['mean'][()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters, forward_fn, tokenizer, config = get_pretrained_model(\n",
    "    model_name=model_name,\n",
    "    mixed_precision=False,\n",
    "    embeddings_layers_to_save=(24,),\n",
    "    attention_maps_to_save=(),\n",
    "    max_positions=41,\n",
    ")\n",
    "forward_fn = hk.transform(forward_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 35/55989 [00:21<9:34:25,  1.62it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m token_id \u001b[39m=\u001b[39m [b[\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m b \u001b[39min\u001b[39;00m token_out]\n\u001b[1;32m     10\u001b[0m seq_pair \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39masarray(token_id,dtype\u001b[39m=\u001b[39mjnp\u001b[39m.\u001b[39mint32)\n\u001b[0;32m---> 11\u001b[0m outs \u001b[39m=\u001b[39m forward_fn\u001b[39m.\u001b[39;49mapply(parameters, random_key, seq_pair)\n\u001b[1;32m     12\u001b[0m lenti_embed\u001b[39m.\u001b[39mappend(outs[\u001b[39m'\u001b[39m\u001b[39membeddings_24\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m])\n",
      "File \u001b[0;32m~/mambaforge/envs/jax-jk/lib/python3.11/site-packages/haiku/_src/transform.py:128\u001b[0m, in \u001b[0;36mwithout_state.<locals>.apply_fn\u001b[0;34m(params, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mstate\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m kwargs:\n\u001b[1;32m    122\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    123\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mHaiku transform adds three arguments (params, state, rng) to apply. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    124\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mIf the functions you are transforming use the same names you must \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    125\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mpass them positionally (e.g. `f.apply(.., my_state)` and not by \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    126\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mname (e.g. `f.apply(.., state=my_state)`)\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 128\u001b[0m out, state \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39;49mapply(params, {}, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    129\u001b[0m \u001b[39mif\u001b[39;00m state:\n\u001b[1;32m    130\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mIf your transformed function uses `hk.\u001b[39m\u001b[39m{\u001b[39m\u001b[39mget,set}_state` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    131\u001b[0m                    \u001b[39m\"\u001b[39m\u001b[39mthen use `hk.transform_with_state`.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/mambaforge/envs/jax-jk/lib/python3.11/site-packages/haiku/_src/transform.py:357\u001b[0m, in \u001b[0;36mtransform_with_state.<locals>.apply_fn\u001b[0;34m(params, state, rng, *args, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[39mwith\u001b[39;00m base\u001b[39m.\u001b[39mnew_context(params\u001b[39m=\u001b[39mparams, state\u001b[39m=\u001b[39mstate, rng\u001b[39m=\u001b[39mrng) \u001b[39mas\u001b[39;00m ctx:\n\u001b[1;32m    356\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 357\u001b[0m     out \u001b[39m=\u001b[39m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    358\u001b[0m   \u001b[39mexcept\u001b[39;00m jax\u001b[39m.\u001b[39merrors\u001b[39m.\u001b[39mUnexpectedTracerError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    359\u001b[0m     \u001b[39mraise\u001b[39;00m jax\u001b[39m.\u001b[39merrors\u001b[39m.\u001b[39mUnexpectedTracerError(unexpected_tracer_hint) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/jax-jk/lib/python3.11/site-packages/nucleotide_transformer/model.py:363\u001b[0m, in \u001b[0;36mbuild_nucleotide_transformer_fn.<locals>.nucleotide_transformer_fn\u001b[0;34m(tokens, attention_mask)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[39m# Run the encoder over the inputs.\u001b[39;00m\n\u001b[1;32m    362\u001b[0m encoder \u001b[39m=\u001b[39m NucleotideTransformer(config\u001b[39m=\u001b[39mmodel_config, name\u001b[39m=\u001b[39mmodel_name)\n\u001b[0;32m--> 363\u001b[0m outs \u001b[39m=\u001b[39m encoder(\n\u001b[1;32m    364\u001b[0m     tokens\u001b[39m=\u001b[39;49mtokens,\n\u001b[1;32m    365\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    366\u001b[0m )\n\u001b[1;32m    367\u001b[0m \u001b[39mreturn\u001b[39;00m outs\n",
      "File \u001b[0;32m~/mambaforge/envs/jax-jk/lib/python3.11/site-packages/haiku/_src/module.py:426\u001b[0m, in \u001b[0;36mwrap_method.<locals>.wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m   \u001b[39mif\u001b[39;00m method_name \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__call__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    424\u001b[0m     f \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mnamed_call(f, name\u001b[39m=\u001b[39mmethod_name)\n\u001b[0;32m--> 426\u001b[0m out \u001b[39m=\u001b[39m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    428\u001b[0m \u001b[39m# Module names are set in the constructor. If `f` is the constructor then\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \u001b[39m# its name will only be set **after** `f` has run. For methods other\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[39m# than `__init__` we need the name before running in order to wrap their\u001b[39;00m\n\u001b[1;32m    431\u001b[0m \u001b[39m# execution with `named_call`.\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[39mif\u001b[39;00m module_name \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/mambaforge/envs/jax-jk/lib/python3.11/contextlib.py:81\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[1;32m     79\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds):\n\u001b[1;32m     80\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 81\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n",
      "File \u001b[0;32m~/mambaforge/envs/jax-jk/lib/python3.11/site-packages/haiku/_src/module.py:272\u001b[0m, in \u001b[0;36mrun_interceptors\u001b[0;34m(bound_method, method_name, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Runs any method interceptors or the original method.\"\"\"\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m interceptor_stack:\n\u001b[0;32m--> 272\u001b[0m   \u001b[39mreturn\u001b[39;00m bound_method(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    274\u001b[0m ctx \u001b[39m=\u001b[39m MethodContext(module\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m,\n\u001b[1;32m    275\u001b[0m                     method_name\u001b[39m=\u001b[39mmethod_name,\n\u001b[1;32m    276\u001b[0m                     orig_method\u001b[39m=\u001b[39mbound_method)\n\u001b[1;32m    277\u001b[0m interceptor_stack_copy \u001b[39m=\u001b[39m interceptor_stack\u001b[39m.\u001b[39mclone()\n",
      "File \u001b[0;32m~/mambaforge/envs/jax-jk/lib/python3.11/site-packages/nucleotide_transformer/model.py:310\u001b[0m, in \u001b[0;36mNucleotideTransformer.__call__\u001b[0;34m(self, tokens, attention_mask)\u001b[0m\n\u001b[1;32m    305\u001b[0m     attention_mask \u001b[39m=\u001b[39m build_padding_attention_mask(\n\u001b[1;32m    306\u001b[0m         tokens\u001b[39m=\u001b[39mtokens, pad_token_id\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_config\u001b[39m.\u001b[39mpad_token_id\n\u001b[1;32m    307\u001b[0m     )\n\u001b[1;32m    309\u001b[0m \u001b[39m# construct a tower of attention layers\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m x, outs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_attention_blocks(\n\u001b[1;32m    311\u001b[0m     x\u001b[39m=\u001b[39;49mx,\n\u001b[1;32m    312\u001b[0m     outs\u001b[39m=\u001b[39;49mouts,\n\u001b[1;32m    313\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    314\u001b[0m )\n\u001b[1;32m    316\u001b[0m \u001b[39m# Language Model Head\u001b[39;00m\n\u001b[1;32m    317\u001b[0m lm_head_outs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lm_head(x)\n",
      "File \u001b[0;32m~/mambaforge/envs/jax-jk/lib/python3.11/site-packages/nucleotide_transformer/model.py:226\u001b[0m, in \u001b[0;36mNucleotideTransformer.apply_attention_blocks\u001b[0;34m(self, x, outs, attention_mask)\u001b[0m\n\u001b[1;32m    224\u001b[0m     layers \u001b[39m=\u001b[39m [hk\u001b[39m.\u001b[39mremat(layer) \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m layers]\n\u001b[1;32m    225\u001b[0m \u001b[39mfor\u001b[39;00m layer_idx, layer \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(layers):\n\u001b[0;32m--> 226\u001b[0m     output \u001b[39m=\u001b[39m layer(\n\u001b[1;32m    227\u001b[0m         x\u001b[39m=\u001b[39;49mx,\n\u001b[1;32m    228\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    229\u001b[0m     )\n\u001b[1;32m    230\u001b[0m     x \u001b[39m=\u001b[39m output[\u001b[39m\"\u001b[39m\u001b[39membeddings\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    231\u001b[0m     \u001b[39m# Save intermediate embeddings if needed\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/jax-jk/lib/python3.11/site-packages/haiku/_src/module.py:426\u001b[0m, in \u001b[0;36mwrap_method.<locals>.wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m   \u001b[39mif\u001b[39;00m method_name \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__call__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    424\u001b[0m     f \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mnamed_call(f, name\u001b[39m=\u001b[39mmethod_name)\n\u001b[0;32m--> 426\u001b[0m out \u001b[39m=\u001b[39m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    428\u001b[0m \u001b[39m# Module names are set in the constructor. If `f` is the constructor then\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \u001b[39m# its name will only be set **after** `f` has run. For methods other\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[39m# than `__init__` we need the name before running in order to wrap their\u001b[39;00m\n\u001b[1;32m    431\u001b[0m \u001b[39m# execution with `named_call`.\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[39mif\u001b[39;00m module_name \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/mambaforge/envs/jax-jk/lib/python3.11/contextlib.py:81\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[1;32m     79\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds):\n\u001b[1;32m     80\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 81\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n",
      "File \u001b[0;32m~/mambaforge/envs/jax-jk/lib/python3.11/site-packages/haiku/_src/module.py:272\u001b[0m, in \u001b[0;36mrun_interceptors\u001b[0;34m(bound_method, method_name, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Runs any method interceptors or the original method.\"\"\"\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m interceptor_stack:\n\u001b[0;32m--> 272\u001b[0m   \u001b[39mreturn\u001b[39;00m bound_method(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    274\u001b[0m ctx \u001b[39m=\u001b[39m MethodContext(module\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m,\n\u001b[1;32m    275\u001b[0m                     method_name\u001b[39m=\u001b[39mmethod_name,\n\u001b[1;32m    276\u001b[0m                     orig_method\u001b[39m=\u001b[39mbound_method)\n\u001b[1;32m    277\u001b[0m interceptor_stack_copy \u001b[39m=\u001b[39m interceptor_stack\u001b[39m.\u001b[39mclone()\n",
      "File \u001b[0;32m~/mambaforge/envs/jax-jk/lib/python3.11/site-packages/nucleotide_transformer/layers.py:281\u001b[0m, in \u001b[0;36mSelfAttentionBlock.__call__\u001b[0;34m(self, x, attention_mask)\u001b[0m\n\u001b[1;32m    279\u001b[0m res \u001b[39m=\u001b[39m x\n\u001b[1;32m    280\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_norm_self_attention(x)\n\u001b[0;32m--> 281\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attention(\n\u001b[1;32m    282\u001b[0m     x\u001b[39m=\u001b[39;49mx,\n\u001b[1;32m    283\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    284\u001b[0m )\n\u001b[1;32m    285\u001b[0m x \u001b[39m=\u001b[39m output[\u001b[39m\"\u001b[39m\u001b[39membeddings\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    286\u001b[0m x \u001b[39m=\u001b[39m res \u001b[39m+\u001b[39m x\n",
      "File \u001b[0;32m~/mambaforge/envs/jax-jk/lib/python3.11/site-packages/nucleotide_transformer/layers.py:240\u001b[0m, in \u001b[0;36mSelfAttentionBlock.self_attention\u001b[0;34m(self, x, attention_mask)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[39m@hk\u001b[39m\u001b[39m.\u001b[39mtransparent\n\u001b[1;32m    224\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mself_attention\u001b[39m(\n\u001b[1;32m    225\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    226\u001b[0m     x: Embedding,\n\u001b[1;32m    227\u001b[0m     attention_mask: Optional[AttentionMask] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    228\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m TransformerOutput:\n\u001b[1;32m    229\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[39m    Applies the self attention mechanism.\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[39m        Dictionary containing the output embeddings and the attention weights.\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 240\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msa_layer(x, x, x, attention_mask\u001b[39m=\u001b[39;49mattention_mask)\n",
      "File \u001b[0;32m~/mambaforge/envs/jax-jk/lib/python3.11/site-packages/haiku/_src/module.py:426\u001b[0m, in \u001b[0;36mwrap_method.<locals>.wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m   \u001b[39mif\u001b[39;00m method_name \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__call__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    424\u001b[0m     f \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mnamed_call(f, name\u001b[39m=\u001b[39mmethod_name)\n\u001b[0;32m--> 426\u001b[0m out \u001b[39m=\u001b[39m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    428\u001b[0m \u001b[39m# Module names are set in the constructor. If `f` is the constructor then\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \u001b[39m# its name will only be set **after** `f` has run. For methods other\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[39m# than `__init__` we need the name before running in order to wrap their\u001b[39;00m\n\u001b[1;32m    431\u001b[0m \u001b[39m# execution with `named_call`.\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[39mif\u001b[39;00m module_name \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/mambaforge/envs/jax-jk/lib/python3.11/contextlib.py:81\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[1;32m     79\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds):\n\u001b[1;32m     80\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 81\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n",
      "File \u001b[0;32m~/mambaforge/envs/jax-jk/lib/python3.11/site-packages/haiku/_src/module.py:272\u001b[0m, in \u001b[0;36mrun_interceptors\u001b[0;34m(bound_method, method_name, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Runs any method interceptors or the original method.\"\"\"\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m interceptor_stack:\n\u001b[0;32m--> 272\u001b[0m   \u001b[39mreturn\u001b[39;00m bound_method(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    274\u001b[0m ctx \u001b[39m=\u001b[39m MethodContext(module\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m,\n\u001b[1;32m    275\u001b[0m                     method_name\u001b[39m=\u001b[39mmethod_name,\n\u001b[1;32m    276\u001b[0m                     orig_method\u001b[39m=\u001b[39mbound_method)\n\u001b[1;32m    277\u001b[0m interceptor_stack_copy \u001b[39m=\u001b[39m interceptor_stack\u001b[39m.\u001b[39mclone()\n",
      "File \u001b[0;32m~/mambaforge/envs/jax-jk/lib/python3.11/site-packages/nucleotide_transformer/layers.py:149\u001b[0m, in \u001b[0;36mMultiHeadAttention.__call__\u001b[0;34m(self, query, key, value, attention_mask)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    130\u001b[0m     query: jnp\u001b[39m.\u001b[39mndarray,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    133\u001b[0m     attention_mask: Optional[jnp\u001b[39m.\u001b[39mndarray] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    134\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m TransformerOutput:\n\u001b[1;32m    135\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[39m    Computes both the embeddings and the attention weights.\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[39m        Dictionary containing the output embeddings and the attention weights.\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 149\u001b[0m     attention_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention_weights(query, key, attention_mask)\n\u001b[1;32m    150\u001b[0m     embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_embeddings(value, attention_weights)\n\u001b[1;32m    151\u001b[0m     \u001b[39mreturn\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39membeddings\u001b[39m\u001b[39m\"\u001b[39m: embeddings, \u001b[39m\"\u001b[39m\u001b[39mattention_weights\u001b[39m\u001b[39m\"\u001b[39m: attention_weights}\n",
      "File \u001b[0;32m~/mambaforge/envs/jax-jk/lib/python3.11/site-packages/nucleotide_transformer/layers.py:88\u001b[0m, in \u001b[0;36mMultiHeadAttention.attention_weights\u001b[0;34m(self, query, key, attention_mask)\u001b[0m\n\u001b[1;32m     86\u001b[0m attention_logits \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39meinsum(\u001b[39m\"\u001b[39m\u001b[39m...thd,...Thd->...htT\u001b[39m\u001b[39m\"\u001b[39m, query_heads, key_heads)\n\u001b[1;32m     87\u001b[0m sqrt_key_size \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39msqrt(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey_size)\u001b[39m.\u001b[39mastype(query\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m---> 88\u001b[0m attention_logits \u001b[39m=\u001b[39m attention_logits \u001b[39m/\u001b[39;49m sqrt_key_size\n\u001b[1;32m     90\u001b[0m \u001b[39mif\u001b[39;00m attention_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     91\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(attention_mask\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(attention_logits\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/mambaforge/envs/jax-jk/lib/python3.11/site-packages/jax/_src/numpy/array_methods.py:251\u001b[0m, in \u001b[0;36m_defer_to_unrecognized_arg.<locals>.deferring_binary_op\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    249\u001b[0m args \u001b[39m=\u001b[39m (other, \u001b[39mself\u001b[39m) \u001b[39mif\u001b[39;00m swap \u001b[39melse\u001b[39;00m (\u001b[39mself\u001b[39m, other)\n\u001b[1;32m    250\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(other, _accepted_binop_types):\n\u001b[0;32m--> 251\u001b[0m   \u001b[39mreturn\u001b[39;00m binary_op(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    252\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(other, _rejected_binop_types):\n\u001b[1;32m    253\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39munsupported operand type(s) for \u001b[39m\u001b[39m{\u001b[39;00mopchar\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    254\u001b[0m                   \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(args[\u001b[39m0\u001b[39m])\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m!r}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(args[\u001b[39m1\u001b[39m])\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m!r}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#get embedding per input sequence\n",
    "random_key = jax.random.PRNGKey(0)\n",
    "N,  = sequence.shape\n",
    "lenti_embed=[]\n",
    "for i in tqdm(range(N)):\n",
    "    seq = sequence[i].decode()\n",
    "    token_out = tokenizer.batch_tokenize([seq])\n",
    "    token_id = [b[1] for b in token_out]\n",
    "    seq_pair = jnp.asarray(token_id,dtype=jnp.int32)\n",
    "    outs = forward_fn.apply(parameters, random_key, seq_pair)\n",
    "    lenti_embed.append(outs['embeddings_24'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_embed = h5py.File('/home/amber/multitask_RNA/data/lenti_MPRA/WTC11_data_embed.h5', 'w')\n",
    "out_embed.create_dataset('embed', data=np.array(lenti_embed))\n",
    "out_embed.create_dataset('target', data=np.array(target))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
